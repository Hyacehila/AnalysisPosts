"""
èˆ†æƒ…åˆ†ææ™ºèƒ½ä½“ - èŠ‚ç‚¹å®šä¹‰

æ ¹æ®è®¾è®¡æ–‡æ¡£ï¼Œç³»ç»Ÿé‡‡ç”¨ä¸­å¤®è°ƒåº¦+ä¸‰é˜¶æ®µé¡ºåºä¾èµ–æ¶æ„ã€‚
æœ¬æ–‡ä»¶åŒ…å«æ‰€æœ‰èŠ‚ç‚¹å®šä¹‰ï¼ŒæŒ‰ä»¥ä¸‹ç»“æ„ç»„ç»‡ï¼š

================================================================================
ç›®å½•ç»“æ„
================================================================================

1. ç³»ç»Ÿè°ƒåº¦èŠ‚ç‚¹
   - DispatcherNode: ç»¼åˆè°ƒåº¦èŠ‚ç‚¹ï¼Œç³»ç»Ÿå…¥å£å’Œä¸­å¤®æ§åˆ¶å™¨
   - TerminalNode: ç»ˆæ­¢èŠ‚ç‚¹ï¼Œå®£å¸ƒæµç¨‹ç»“æŸ

2. åŸºç¡€èŠ‚ç‚¹ç±»
   - AsyncParallelBatchNode: å¸¦å¹¶å‘é™åˆ¶çš„å¼‚æ­¥å¹¶è¡Œæ‰¹å¤„ç†åŸºç±»

3. é˜¶æ®µ1èŠ‚ç‚¹: åŸå§‹åšæ–‡å¢å¼ºå¤„ç†
   3.1 é€šç”¨èŠ‚ç‚¹
       - DataLoadNode: æ•°æ®åŠ è½½
       - SaveEnhancedDataNode: ä¿å­˜å¢å¼ºæ•°æ®
       - DataValidationAndOverviewNode: æ•°æ®éªŒè¯ä¸æ¦‚å†µåˆ†æ
       - Stage1CompletionNode: é˜¶æ®µ1å®ŒæˆèŠ‚ç‚¹ï¼Œè¿”å›è°ƒåº¦å™¨
   3.2 å¼‚æ­¥æ‰¹é‡å¹¶è¡Œè·¯å¾„èŠ‚ç‚¹ (enhancement_mode="async")
       - AsyncSentimentPolarityAnalysisBatchNode: æƒ…æ„Ÿææ€§åˆ†æ
       - AsyncSentimentAttributeAnalysisBatchNode: æƒ…æ„Ÿå±æ€§åˆ†æ
       - AsyncTwoLevelTopicAnalysisBatchNode: ä¸¤çº§ä¸»é¢˜åˆ†æ
       - AsyncPublisherObjectAnalysisBatchNode: å‘å¸ƒè€…å¯¹è±¡åˆ†æ
   3.3 Batch APIè·¯å¾„èŠ‚ç‚¹ (enhancement_mode="batch_api")
       - BatchAPIEnhancementNode: è°ƒç”¨Batch APIè„šæœ¬å¤„ç†

4. é˜¶æ®µ2èŠ‚ç‚¹: åˆ†ææ‰§è¡Œï¼ˆå¾…å®ç°ï¼‰
   - Stage2EntryNode, WorkflowAnalysisNode, AgentAnalysisFlowèŠ‚ç‚¹ç­‰

5. é˜¶æ®µ3èŠ‚ç‚¹: æŠ¥å‘Šç”Ÿæˆï¼ˆå¾…å®ç°ï¼‰
   - Stage3EntryNode, TemplateReportNode, IterativeReportFlowèŠ‚ç‚¹ç­‰

================================================================================
"""

import json
import os
import asyncio
import subprocess
from typing import List, Dict, Any, Optional
from pocketflow import Node, BatchNode, AsyncNode
from utils.call_llm import call_glm_45_air, call_glm4v_plus
from utils.data_loader import (
    load_blog_data, load_topics, load_sentiment_attributes, 
    load_publisher_objects, save_enhanced_blog_data, load_enhanced_blog_data
)


# =============================================================================
# 1. ç³»ç»Ÿè°ƒåº¦èŠ‚ç‚¹
# =============================================================================

class DispatcherNode(Node):
    """
    ç»¼åˆè°ƒåº¦èŠ‚ç‚¹ - ç³»ç»Ÿå…¥å£å’Œä¸­å¤®æ§åˆ¶å™¨
    
    åŠŸèƒ½ï¼š
    1. ä½œä¸ºæ•´ä¸ªç³»ç»ŸFlowçš„å…¥å£èŠ‚ç‚¹
    2. æ ¹æ®shared["dispatcher"]é…ç½®å†³å®šæ‰§è¡Œå“ªä¸ªé˜¶æ®µ
    3. æ ¹æ®å„é˜¶æ®µçš„configå‚æ•°å†³å®šå…·ä½“æ‰§è¡Œè·¯å¾„
    4. æ¯ä¸ªé˜¶æ®µå®Œæˆåè¿”å›æ­¤èŠ‚ç‚¹ï¼Œå†³å®šä¸‹ä¸€æ­¥åŠ¨ä½œ
    
    è¿”å›çš„Actionç±»å‹ï¼š
    - stage1_async: é˜¶æ®µ1å¼‚æ­¥å¤„ç†è·¯å¾„
    - stage1_batch_api: é˜¶æ®µ1 Batch APIå¤„ç†è·¯å¾„
    - stage2_workflow: é˜¶æ®µ2å›ºå®šè„šæœ¬åˆ†æ
    - stage2_agent: é˜¶æ®µ2 LLMè‡ªä¸»åˆ†æ
    - stage3_template: é˜¶æ®µ3æ¨¡æ¿å¡«å……
    - stage3_iterative: é˜¶æ®µ3å¤šè½®è¿­ä»£
    - done: æ‰€æœ‰é˜¶æ®µå®Œæˆï¼Œè·³è½¬åˆ°TerminalNode
    """
    
    def prep(self, shared):
        """è¯»å–è°ƒåº¦é…ç½®å’Œå½“å‰çŠ¶æ€"""
        # åˆå§‹åŒ–dispatcheré…ç½®ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        if "dispatcher" not in shared:
            shared["dispatcher"] = {
                "start_stage": 1,
                "run_stages": [1, 2, 3],
                "current_stage": 0,
                "completed_stages": [],
                "next_action": None
            }
        
        dispatcher = shared["dispatcher"]
        config = shared.get("config", {})
        
        return {
            "start_stage": dispatcher.get("start_stage", 1),
            "run_stages": dispatcher.get("run_stages", [1, 2, 3]),
            "current_stage": dispatcher.get("current_stage", 0),
            "completed_stages": dispatcher.get("completed_stages", []),
            "enhancement_mode": config.get("enhancement_mode", "async"),
            "analysis_mode": config.get("analysis_mode", "workflow"),
            "report_mode": config.get("report_mode", "template")
        }
    
    def exec(self, prep_res):
        """è®¡ç®—ä¸‹ä¸€æ­¥åŠ¨ä½œ"""
        start_stage = prep_res["start_stage"]
        run_stages = prep_res["run_stages"]
        current_stage = prep_res["current_stage"]
        completed_stages = prep_res["completed_stages"]
        enhancement_mode = prep_res["enhancement_mode"]
        analysis_mode = prep_res["analysis_mode"]
        report_mode = prep_res["report_mode"]
        
        # ç¡®å®šä¸‹ä¸€ä¸ªéœ€è¦æ‰§è¡Œçš„é˜¶æ®µ
        if current_stage == 0:
            # é¦–æ¬¡è¿›å…¥ï¼Œä»start_stageå¼€å§‹
            next_stage = start_stage
        else:
            # æ‰¾åˆ°ä¸‹ä¸€ä¸ªåœ¨run_stagesä¸­ä¸”æœªå®Œæˆçš„é˜¶æ®µ
            next_stage = None
            for stage in run_stages:
                if stage > current_stage and stage not in completed_stages:
                    next_stage = stage
                    break
        
        # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰éœ€è¦æ‰§è¡Œçš„é˜¶æ®µ
        if next_stage is None or next_stage not in run_stages:
            return {"action": "done", "next_stage": None}
        
        # æ ¹æ®é˜¶æ®µç¡®å®šå…·ä½“è·¯å¾„
        if next_stage == 1:
            action = f"stage1_{enhancement_mode}"
        elif next_stage == 2:
            action = f"stage2_{analysis_mode}"
        elif next_stage == 3:
            action = f"stage3_{report_mode}"
        else:
            action = "done"
        
        return {"action": action, "next_stage": next_stage}
    
    def post(self, shared, prep_res, exec_res):
        """æ›´æ–°è°ƒåº¦çŠ¶æ€ï¼Œè¿”å›Action"""
        action = exec_res["action"]
        next_stage = exec_res["next_stage"]
        
        # æ›´æ–°å½“å‰é˜¶æ®µ
        if next_stage is not None:
            shared["dispatcher"]["current_stage"] = next_stage
        
        shared["dispatcher"]["next_action"] = action
        
        print(f"[Dispatcher] ä¸‹ä¸€æ­¥åŠ¨ä½œ: {action}")
        
        return action


class TerminalNode(Node):
    """
    ç»ˆæ­¢èŠ‚ç‚¹ - å®£å¸ƒæµç¨‹ç»“æŸ
    
    åŠŸèƒ½ï¼š
    1. ä½œä¸ºæ•´ä¸ªFlowçš„ç»ˆç‚¹
    2. è¾“å‡ºæ‰§è¡Œæ‘˜è¦ä¿¡æ¯
    3. æ¸…ç†ä¸´æ—¶çŠ¶æ€ï¼ˆå¦‚éœ€è¦ï¼‰
    """
    
    def prep(self, shared):
        """è¯»å–æ‰§è¡Œç»“æœæ‘˜è¦"""
        dispatcher = shared.get("dispatcher", {})
        stage1_results = shared.get("stage1_results", {})
        
        return {
            "completed_stages": dispatcher.get("completed_stages", []),
            "statistics": stage1_results.get("statistics", {}),
            "data_save": stage1_results.get("data_save", {})
        }
    
    def exec(self, prep_res):
        """ç”Ÿæˆæ‰§è¡Œæ‘˜è¦"""
        completed_stages = prep_res["completed_stages"]
        statistics = prep_res["statistics"]
        data_save = prep_res["data_save"]
        
        summary = {
            "status": "completed",
            "completed_stages": completed_stages,
            "total_blogs_processed": statistics.get("total_blogs", 0),
            "data_saved": data_save.get("saved", False),
            "output_path": data_save.get("output_path", "")
        }
        
        return summary
    
    def post(self, shared, prep_res, exec_res):
        """è¾“å‡ºæ‰§è¡Œæ‘˜è¦ï¼Œç»“æŸæµç¨‹"""
        print("\n" + "=" * 60)
        print("èˆ†æƒ…åˆ†ææ™ºèƒ½ä½“ - æ‰§è¡Œå®Œæˆ")
        print("=" * 60)
        print(f"çŠ¶æ€: {exec_res['status']}")
        print(f"å·²å®Œæˆé˜¶æ®µ: {exec_res['completed_stages']}")
        print(f"å¤„ç†åšæ–‡æ•°: {exec_res['total_blogs_processed']}")
        if exec_res['data_saved']:
            print(f"æ•°æ®å·²ä¿å­˜è‡³: {exec_res['output_path']}")
        print("=" * 60 + "\n")
        
        # å­˜å‚¨æœ€ç»ˆæ‘˜è¦
        shared["final_summary"] = exec_res
        
        return "default"


# =============================================================================
# 2. åŸºç¡€èŠ‚ç‚¹ç±»
# =============================================================================

class AsyncParallelBatchNode(AsyncNode, BatchNode):
    """
    å¸¦å¹¶å‘é™åˆ¶çš„å¼‚æ­¥å¹¶è¡Œæ‰¹å¤„ç†èŠ‚ç‚¹
    
    ç”¨äºé˜¶æ®µ1çš„å¼‚æ­¥æ‰¹é‡å¹¶è¡Œå¤„ç†è·¯å¾„ (enhancement_mode="async")
    æ”¯æŒé€šè¿‡ max_concurrent å‚æ•°æ§åˆ¶å¹¶å‘æ‰§è¡Œæ•°é‡ï¼Œé¿å…è§¦å‘APIé™æµ
    """
    
    def __init__(self, max_concurrent: Optional[int] = None, **kwargs):
        """
        åˆå§‹åŒ–å¼‚æ­¥å¹¶è¡Œæ‰¹å¤„ç†èŠ‚ç‚¹
        
        Args:
            max_concurrent: æœ€å¤§å¹¶å‘æ•°ï¼ŒNoneè¡¨ç¤ºä¸é™åˆ¶
        """
        super().__init__(**kwargs)
        self.max_concurrent = max_concurrent
        # åœ¨æ„é€ æ—¶åˆ›å»ºä¿¡å·é‡ï¼ˆå®ä¾‹çº§åˆ«å…±äº«ï¼‰
        self._semaphore = (
            asyncio.Semaphore(max_concurrent) 
            if max_concurrent else None
        )
    
    async def _exec(self, items):
        """æ‰§è¡Œæ‰¹é‡å¤„ç†ï¼Œæ”¯æŒå¹¶å‘æ§åˆ¶"""
        if not items:
            return []
        
        if self._semaphore:
            async def sem_exec(item):
                async with self._semaphore:
                    return await AsyncNode._exec(self, item)
            
            return await asyncio.gather(*(sem_exec(i) for i in items))
        else:
            return await asyncio.gather(*(AsyncNode._exec(self, i) for i in items))


# =============================================================================
# 3. é˜¶æ®µ1èŠ‚ç‚¹: åŸå§‹åšæ–‡å¢å¼ºå¤„ç†
# =============================================================================

# -----------------------------------------------------------------------------
# 3.1 é€šç”¨èŠ‚ç‚¹
# -----------------------------------------------------------------------------

class DataLoadNode(Node):
    """
    æ•°æ®åŠ è½½èŠ‚ç‚¹
    
    åŠŸèƒ½ï¼šåŠ è½½åŸå§‹åšæ–‡æ•°æ®æˆ–å·²å¢å¼ºçš„æ•°æ®
    ç±»å‹ï¼šRegular Node
    
    æ ¹æ® config.data_source.type é…ç½®å†³å®šåŠ è½½æ–¹å¼ï¼š
    - "original": åŠ è½½åŸå§‹åšæ–‡æ•°æ®åŠå‚è€ƒæ•°æ®
    - "enhanced": åŠ è½½å·²å¢å¼ºçš„åšæ–‡æ•°æ®
    """
    
    def prep(self, shared):
        """è¯»å–æ•°æ®æ–‡ä»¶è·¯å¾„å’Œé…ç½®å‚æ•°"""
        config = shared.get("config", {})
        data_paths = shared.get("data", {}).get("data_paths", {})
        
        data_source_type = config.get("data_source", {}).get("type", "original")
        
        if data_source_type == "enhanced":
            enhanced_data_path = config.get("data_source", {}).get(
                "enhanced_data_path", "data/enhanced_blogs.json"
            )
            return {
                "load_type": "enhanced",
                "data_path": enhanced_data_path
            }
        else:
            return {
                "load_type": "original",
                "blog_data_path": data_paths.get("blog_data_path", "data/beijing_rainstorm_posts.json"),
                "topics_path": data_paths.get("topics_path", "data/topics.json"),
                "sentiment_attributes_path": data_paths.get("sentiment_attributes_path", "data/sentiment_attributes.json"),
                "publisher_objects_path": data_paths.get("publisher_objects_path", "data/publisher_objects.json")
            }
    
    def exec(self, prep_res):
        """åŠ è½½JSONæ ¼å¼æ•°æ®ï¼ŒéªŒè¯æ ¼å¼å®Œæ•´æ€§"""
        if prep_res["load_type"] == "enhanced":
            enhanced_data = load_enhanced_blog_data(prep_res["data_path"])
            return {
                "blog_data": enhanced_data,
                "load_type": "enhanced"
            }
        else:
            return {
                "blog_data": load_blog_data(prep_res["blog_data_path"]),
                "topics_hierarchy": load_topics(prep_res["topics_path"]),
                "sentiment_attributes": load_sentiment_attributes(prep_res["sentiment_attributes_path"]),
                "publisher_objects": load_publisher_objects(prep_res["publisher_objects_path"]),
                "load_type": "original"
            }
    
    def post(self, shared, prep_res, exec_res):
        """å°†æ•°æ®å­˜å‚¨åˆ°sharedä¸­"""
        if "data" not in shared:
            shared["data"] = {}
        
        shared["data"]["blog_data"] = exec_res["blog_data"]
        shared["data"]["load_type"] = exec_res["load_type"]
        
        if exec_res["load_type"] == "original":
            shared["data"]["topics_hierarchy"] = exec_res["topics_hierarchy"]
            shared["data"]["sentiment_attributes"] = exec_res["sentiment_attributes"]
            shared["data"]["publisher_objects"] = exec_res["publisher_objects"]
        
        if "stage1_results" not in shared:
            shared["stage1_results"] = {"statistics": {}}
        shared["stage1_results"]["statistics"]["total_blogs"] = len(exec_res["blog_data"])
        
        print(f"[DataLoad] åŠ è½½å®Œæˆï¼Œå…± {len(exec_res['blog_data'])} æ¡åšæ–‡")
        
        return "default"


class SaveEnhancedDataNode(Node):
    """
    å¢å¼ºæ•°æ®ä¿å­˜èŠ‚ç‚¹
    
    åŠŸèƒ½ï¼šå°†å¢å¼ºåçš„åšæ–‡æ•°æ®ä¿å­˜åˆ°æŒ‡å®šæ–‡ä»¶è·¯å¾„
    ç±»å‹ï¼šRegular Node
    è¾“å‡ºï¼šdata/enhanced_posts.jsonï¼ˆé˜¶æ®µ1è¾“å‡ºï¼Œä¾›é˜¶æ®µ2ä½¿ç”¨ï¼‰
    """
    
    def prep(self, shared):
        """è¯»å–å¢å¼ºåçš„åšæ–‡æ•°æ®å’Œä¿å­˜è·¯å¾„é…ç½®"""
        blog_data = shared.get("data", {}).get("blog_data", [])
        config = shared.get("config", {})
        output_path = config.get("data_source", {}).get(
            "enhanced_data_path", "data/enhanced_blogs.json"
        )
        
        return {
            "blog_data": blog_data,
            "output_path": output_path
        }
    
    def exec(self, prep_res):
        """è°ƒç”¨æ•°æ®ä¿å­˜å·¥å…·å‡½æ•°ï¼Œå°†å¢å¼ºæ•°æ®å†™å…¥æ–‡ä»¶"""
        blog_data = prep_res["blog_data"]
        output_path = prep_res["output_path"]
        
        success = save_enhanced_blog_data(blog_data, output_path)
        
        return {
            "success": success,
            "output_path": output_path,
            "data_count": len(blog_data)
        }
    
    def post(self, shared, prep_res, exec_res):
        """éªŒè¯ä¿å­˜ç»“æœï¼Œæ›´æ–°ä¿å­˜çŠ¶æ€ä¿¡æ¯"""
        if "stage1_results" not in shared:
            shared["stage1_results"] = {}
        
        if exec_res["success"]:
            print(f"[SaveData] âœ“ æˆåŠŸä¿å­˜ {exec_res['data_count']} æ¡å¢å¼ºæ•°æ®åˆ°: {exec_res['output_path']}")
            shared["stage1_results"]["data_save"] = {
                "saved": True,
                "output_path": exec_res["output_path"],
                "data_count": exec_res["data_count"]
            }
        else:
            print(f"[SaveData] âœ— ä¿å­˜å¢å¼ºæ•°æ®å¤±è´¥: {exec_res['output_path']}")
            shared["stage1_results"]["data_save"] = {
                "saved": False,
                "output_path": exec_res["output_path"],
                "error": "ä¿å­˜å¤±è´¥"
            }
        
        return "default"


class DataValidationAndOverviewNode(Node):
    """
    æ•°æ®éªŒè¯ä¸æ¦‚å†µåˆ†æèŠ‚ç‚¹
    
    åŠŸèƒ½ï¼šéªŒè¯å¢å¼ºæ•°æ®çš„å®Œæ•´æ€§å¹¶ç”Ÿæˆæ•°æ®ç»Ÿè®¡æ¦‚å†µ
    ç±»å‹ï¼šRegular Node
    ç”¨äºé˜¶æ®µ1å®Œæˆåçš„è´¨é‡æ£€æŸ¥
    """
    
    def prep(self, shared):
        """è¯»å–å¢å¼ºåçš„åšæ–‡æ•°æ®"""
        return shared.get("data", {}).get("blog_data", [])
    
    def exec(self, prep_res):
        """éªŒè¯å¿…éœ€å­—æ®µæ˜¯å¦å­˜åœ¨ï¼Œç»Ÿè®¡ç•™ç©ºå­—æ®µæ•°é‡ï¼Œç”Ÿæˆæ•°æ®ç»Ÿè®¡æ¦‚å†µ"""
        blog_data = prep_res
        
        stats = {
            "total_blogs": len(blog_data),
            "processed_blogs": 0,
            "engagement_statistics": {
                "total_reposts": 0,
                "total_comments": 0,
                "total_likes": 0,
                "avg_reposts": 0,
                "avg_comments": 0,
                "avg_likes": 0
            },
            "user_statistics": {
                "unique_users": set(),
                "top_active_users": [],
                "user_type_distribution": {}
            },
            "content_statistics": {
                "total_images": 0,
                "blogs_with_images": 0,
                "avg_content_length": 0,
                "time_distribution": {}
            },
            "geographic_distribution": {},
            "empty_fields": {
                "sentiment_polarity_empty": 0,
                "sentiment_attribute_empty": 0,
                "topics_empty": 0,
                "publisher_empty": 0
            }
        }
        
        total_content_length = 0
        user_engagement = {}
        
        for blog_post in blog_data:
            # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†
            has_analysis = (
                blog_post.get('sentiment_polarity') is not None or
                blog_post.get('sentiment_attribute') is not None or
                blog_post.get('topics') is not None or
                blog_post.get('publisher') is not None
            )
            if has_analysis:
                stats["processed_blogs"] += 1
            
            # å‚ä¸åº¦ç»Ÿè®¡
            repost_count = blog_post.get('repost_count', 0)
            comment_count = blog_post.get('comment_count', 0)
            like_count = blog_post.get('like_count', 0)
            
            stats["engagement_statistics"]["total_reposts"] += repost_count
            stats["engagement_statistics"]["total_comments"] += comment_count
            stats["engagement_statistics"]["total_likes"] += like_count
            
            # ç”¨æˆ·ç»Ÿè®¡
            user_id = blog_post.get('user_id', '')
            username = blog_post.get('username', '')
            if user_id:
                stats["user_statistics"]["unique_users"].add(user_id)
                if user_id not in user_engagement:
                    user_engagement[user_id] = {"username": username, "total_engagement": 0}
                user_engagement[user_id]["total_engagement"] += repost_count + comment_count + like_count
            
            # å†…å®¹ç»Ÿè®¡
            content = blog_post.get('content', '')
            total_content_length += len(content)
            
            image_urls = blog_post.get('image_urls', [])
            if image_urls:
                stats["content_statistics"]["total_images"] += len(image_urls)
                stats["content_statistics"]["blogs_with_images"] += 1
            
            # æ—¶é—´åˆ†å¸ƒ
            publish_time = blog_post.get('publish_time', '')
            if publish_time:
                try:
                    hour = int(publish_time.split(' ')[1].split(':')[0]) if ' ' in publish_time else 0
                    hour_key = f"{hour:02d}:00"
                    stats["content_statistics"]["time_distribution"][hour_key] = \
                        stats["content_statistics"]["time_distribution"].get(hour_key, 0) + 1
                except:
                    pass
            
            # åœ°ç†åˆ†å¸ƒ
            location = blog_post.get('location', '')
            if location:
                stats["geographic_distribution"][location] = \
                    stats["geographic_distribution"].get(location, 0) + 1
            
            # ç©ºå­—æ®µç»Ÿè®¡
            if blog_post.get('sentiment_polarity') is None:
                stats["empty_fields"]["sentiment_polarity_empty"] += 1
            if blog_post.get('sentiment_attribute') is None:
                stats["empty_fields"]["sentiment_attribute_empty"] += 1
            if blog_post.get('topics') is None:
                stats["empty_fields"]["topics_empty"] += 1
            if blog_post.get('publisher') is None:
                stats["empty_fields"]["publisher_empty"] += 1
            
            # å‘å¸ƒè€…ç±»å‹åˆ†å¸ƒ
            publisher = blog_post.get('publisher')
            if publisher:
                stats["user_statistics"]["user_type_distribution"][publisher] = \
                    stats["user_statistics"]["user_type_distribution"].get(publisher, 0) + 1
        
        # è®¡ç®—å¹³å‡å€¼
        if stats["total_blogs"] > 0:
            stats["engagement_statistics"]["avg_reposts"] = \
                stats["engagement_statistics"]["total_reposts"] / stats["total_blogs"]
            stats["engagement_statistics"]["avg_comments"] = \
                stats["engagement_statistics"]["total_comments"] / stats["total_blogs"]
            stats["engagement_statistics"]["avg_likes"] = \
                stats["engagement_statistics"]["total_likes"] / stats["total_blogs"]
            stats["content_statistics"]["avg_content_length"] = \
                total_content_length / stats["total_blogs"]
        
        # è½¬æ¢setä¸ºæ•°é‡
        stats["user_statistics"]["unique_users"] = len(stats["user_statistics"]["unique_users"])
        
        # æ´»è·ƒç”¨æˆ·æ’è¡Œï¼ˆå‰10ï¼‰
        sorted_users = sorted(
            user_engagement.items(), 
            key=lambda x: x[1]["total_engagement"], 
            reverse=True
        )[:10]
        stats["user_statistics"]["top_active_users"] = [
            {"user_id": uid, "username": info["username"], "total_engagement": info["total_engagement"]}
            for uid, info in sorted_users
        ]
        
        return stats
    
    def post(self, shared, prep_res, exec_res):
        """å°†ç»Ÿè®¡ä¿¡æ¯å­˜å‚¨åˆ°sharedä¸­ï¼Œå¹¶æ‰“å°è¯¦ç»†ç»Ÿè®¡æŠ¥å‘Š"""
        if "stage1_results" not in shared:
            shared["stage1_results"] = {}
        if "statistics" not in shared["stage1_results"]:
            shared["stage1_results"]["statistics"] = {}
        
        shared["stage1_results"]["statistics"].update(exec_res)
        
        # æ‰“å°è¯¦ç»†ç»Ÿè®¡æŠ¥å‘Š
        stats = exec_res
        print("\n" + "=" * 60)
        print("é˜¶æ®µ1 æ•°æ®å¢å¼ºç»Ÿè®¡æŠ¥å‘Š".center(52))
        print("=" * 60)
        
        # åŸºç¡€ç»Ÿè®¡
        print(f"\nğŸ“Š åŸºç¡€ç»Ÿè®¡:")
        print(f"  â”œâ”€ æ€»åšæ–‡æ•°: {stats.get('total_blogs', 0)}")
        print(f"  â””â”€ å·²å¤„ç†æ•°: {stats.get('processed_blogs', 0)}")
        
        # ç©ºå­—æ®µç»Ÿè®¡
        empty_fields = stats.get("empty_fields", {})
        if empty_fields:
            print(f"\nâš ï¸  å¢å¼ºå­—æ®µç©ºå€¼ç»Ÿè®¡:")
            print(f"  â”œâ”€ æƒ…æ„Ÿææ€§ä¸ºç©º: {empty_fields.get('sentiment_polarity_empty', 0)}")
            print(f"  â”œâ”€ æƒ…æ„Ÿå±æ€§ä¸ºç©º: {empty_fields.get('sentiment_attribute_empty', 0)}")
            print(f"  â”œâ”€ ä¸»é¢˜ä¸ºç©º: {empty_fields.get('topics_empty', 0)}")
            print(f"  â””â”€ å‘å¸ƒè€…ä¸ºç©º: {empty_fields.get('publisher_empty', 0)}")
        
        # å‚ä¸åº¦ç»Ÿè®¡
        engagement = stats.get("engagement_statistics", {})
        if engagement:
            print(f"\nğŸ’¬ å‚ä¸åº¦ç»Ÿè®¡:")
            print(f"  â”œâ”€ æ€»è½¬å‘æ•°: {engagement.get('total_reposts', 0)}")
            print(f"  â”œâ”€ æ€»è¯„è®ºæ•°: {engagement.get('total_comments', 0)}")
            print(f"  â”œâ”€ æ€»ç‚¹èµæ•°: {engagement.get('total_likes', 0)}")
            print(f"  â”œâ”€ å¹³å‡è½¬å‘: {engagement.get('avg_reposts', 0):.2f}")
            print(f"  â”œâ”€ å¹³å‡è¯„è®º: {engagement.get('avg_comments', 0):.2f}")
            print(f"  â””â”€ å¹³å‡ç‚¹èµ: {engagement.get('avg_likes', 0):.2f}")
        
        # ç”¨æˆ·ç»Ÿè®¡
        user_stats = stats.get("user_statistics", {})
        if user_stats:
            print(f"\nğŸ‘¥ ç”¨æˆ·ç»Ÿè®¡:")
            print(f"  â”œâ”€ ç‹¬ç«‹ç”¨æˆ·æ•°: {user_stats.get('unique_users', 0)}")
            user_type_dist = user_stats.get('user_type_distribution', {})
            if user_type_dist:
                print(f"  â””â”€ å‘å¸ƒè€…ç±»å‹åˆ†å¸ƒ:")
                for i, (pub_type, count) in enumerate(sorted(user_type_dist.items(), key=lambda x: -x[1])):
                    prefix = "      â”œâ”€" if i < len(user_type_dist) - 1 else "      â””â”€"
                    print(f"{prefix} {pub_type}: {count}")
        
        # å†…å®¹ç»Ÿè®¡
        content_stats = stats.get("content_statistics", {})
        if content_stats:
            print(f"\nğŸ“ å†…å®¹ç»Ÿè®¡:")
            print(f"  â”œâ”€ å«å›¾åšæ–‡æ•°: {content_stats.get('blogs_with_images', 0)}")
            print(f"  â”œâ”€ æ€»å›¾ç‰‡æ•°: {content_stats.get('total_images', 0)}")
            print(f"  â””â”€ å¹³å‡å†…å®¹é•¿åº¦: {content_stats.get('avg_content_length', 0):.1f} å­—ç¬¦")
        
        # åœ°ç†åˆ†å¸ƒï¼ˆå‰5ï¼‰
        geo_dist = stats.get("geographic_distribution", {})
        if geo_dist:
            print(f"\nğŸŒ åœ°ç†åˆ†å¸ƒ (Top 5):")
            sorted_geo = sorted(geo_dist.items(), key=lambda x: -x[1])[:5]
            for i, (location, count) in enumerate(sorted_geo):
                prefix = "  â”œâ”€" if i < len(sorted_geo) - 1 else "  â””â”€"
                print(f"{prefix} {location}: {count}")
        
        print("\n" + "=" * 60 + "\n")
        
        return "default"


class Stage1CompletionNode(Node):
    """
    é˜¶æ®µ1å®ŒæˆèŠ‚ç‚¹
    
    åŠŸèƒ½ï¼š
    1. æ ‡è®°é˜¶æ®µ1å®Œæˆ
    2. æ›´æ–°dispatcherçŠ¶æ€
    3. è¿”å›"dispatch" Actionï¼Œè·³è½¬å›DispatcherNode
    """
    
    def prep(self, shared):
        """è¯»å–å½“å‰çŠ¶æ€"""
        return {
            "current_stage": shared.get("dispatcher", {}).get("current_stage", 1),
            "completed_stages": shared.get("dispatcher", {}).get("completed_stages", [])
        }
    
    def exec(self, prep_res):
        """ç¡®è®¤é˜¶æ®µå®Œæˆ"""
        print(f"\n[Stage1] é˜¶æ®µ1å¤„ç†å®Œæˆ")
        return {"stage": 1}
    
    def post(self, shared, prep_res, exec_res):
        """æ›´æ–°å®ŒæˆçŠ¶æ€ï¼Œè¿”å›dispatch"""
        stage = exec_res["stage"]
        
        # ç¡®ä¿dispatcherå­˜åœ¨
        if "dispatcher" not in shared:
            shared["dispatcher"] = {}
        
        # æ›´æ–°å·²å®Œæˆé˜¶æ®µåˆ—è¡¨
        completed_stages = shared["dispatcher"].get("completed_stages", [])
        if stage not in completed_stages:
            completed_stages.append(stage)
        shared["dispatcher"]["completed_stages"] = completed_stages
        
        print(f"[Stage1] å·²å®Œæˆé˜¶æ®µ: {completed_stages}")
        
        # è¿”å›dispatchï¼Œè·³è½¬å›è°ƒåº¦å™¨
        return "dispatch"


# -----------------------------------------------------------------------------
# 3.2 å¼‚æ­¥æ‰¹é‡å¹¶è¡Œè·¯å¾„èŠ‚ç‚¹ (enhancement_mode="async")
# -----------------------------------------------------------------------------

class AsyncSentimentPolarityAnalysisBatchNode(AsyncParallelBatchNode):
    """
    å¼‚æ­¥æƒ…æ„Ÿææ€§åˆ†æèŠ‚ç‚¹
    
    åŠŸèƒ½ï¼šæ‰¹é‡åˆ†æåšæ–‡çš„æƒ…æ„Ÿææ€§ï¼ˆ1-5æ¡£æ•°å­—åˆ†çº§ï¼‰
    ç±»å‹ï¼šAsyncParallelBatchNode
    è¾“å‡ºå­—æ®µï¼šsentiment_polarity (int: 1-5)
    
    æƒ…æ„Ÿææ€§å®šä¹‰ï¼š
    - 1: æåº¦æ‚²è§‚
    - 2: æ‚²è§‚
    - 3: ä¸­æ€§/æ— æ˜æ˜¾ææ€§
    - 4: ä¹è§‚
    - 5: æåº¦ä¹è§‚
    """
    
    async def prep_async(self, shared):
        """è¿”å›åšæ–‡æ•°æ®åˆ—è¡¨"""
        return shared.get("data", {}).get("blog_data", [])
    
    async def exec_async(self, prep_res):
        """å¯¹å•æ¡åšæ–‡è°ƒç”¨å¤šæ¨¡æ€LLMè¿›è¡Œæƒ…æ„Ÿææ€§åˆ†æ"""
        blog_post = prep_res
        
        prompt = f"""ä½ æ˜¯ç¤¾äº¤åª’ä½“åˆ†æå¸ˆï¼Œè¯·ä¾æ®ä¸‹è¡¨åˆ¤æ–­åšæ–‡æ•´ä½“æƒ…æ„Ÿææ€§ï¼š
- 1=æåº¦æ‚²è§‚ï¼Œ2=æ‚²è§‚ï¼Œ3=ä¸­æ€§ï¼Œ4=ä¹è§‚ï¼Œ5=æåº¦ä¹è§‚ï¼Œ0=æ— æ³•åˆ¤æ–­
- ä»…è¾“å‡ºä¸€ä¸ªæ•°å­—ï¼ˆ0-5ï¼‰ï¼Œä¸å¾—é™„åŠ è§£é‡Šæˆ–å…¶ä»–å­—ç¬¦
åšæ–‡å†…å®¹ï¼š
{blog_post.get('content', '')}"""
        
        # å¤„ç†å›¾ç‰‡è·¯å¾„
        image_paths = blog_post.get('image_urls', [])
        image_paths = [img for img in image_paths if img and img.strip()]
        
        processed_image_paths = []
        for img_path in image_paths:
            if not os.path.isabs(img_path):
                full_path = os.path.join("data", img_path)
                processed_image_paths.append(full_path)
            else:
                processed_image_paths.append(img_path)
        
        # å¼‚æ­¥è°ƒç”¨LLM
        if processed_image_paths:
            response = await asyncio.to_thread(
                call_glm4v_plus, prompt, image_paths=processed_image_paths, temperature=0.3
            )
        else:
            response = await asyncio.to_thread(
                call_glm_45_air, prompt, temperature=0.3
            )
        
        # éªŒè¯ç»“æœ
        response = response.strip()
        if not response.isdigit():
            raise ValueError(f"æ¨¡å‹è¾“å‡ºä¸æ˜¯æ•°å­—: {response}")
        
        score = int(response)
        if not 1 <= score <= 5:
            raise ValueError(f"æ¨¡å‹è¾“å‡ºæ•°å­—ä¸åœ¨1-5èŒƒå›´å†…: {score}")
        
        return score
    
    async def exec_fallback_async(self, prep_res, exc):
        """åˆ†æå¤±è´¥æ—¶è¿”å›ä¸­æ€§è¯„åˆ†"""
        print(f"æƒ…æ„Ÿææ€§åˆ†æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼: {str(exc)}")
        return 3
    
    async def post_async(self, shared, prep_res, exec_res):
        """å°†åˆ†æç»“æœé™„åŠ åˆ°åšæ–‡å¯¹è±¡"""
        blog_data = shared.get("data", {}).get("blog_data", [])
        
        if len(exec_res) != len(blog_data):
            print("è­¦å‘Šï¼šæƒ…æ„Ÿææ€§åˆ†æç»“æœæ•°é‡ä¸åšæ–‡æ•°é‡ä¸åŒ¹é…")
            return "default"
        
        for i, blog_post in enumerate(blog_data):
            blog_post['sentiment_polarity'] = exec_res[i] if i < len(exec_res) else None
        
        print(f"[AsyncSentimentPolarity] å®Œæˆ {len(exec_res)} æ¡åšæ–‡çš„æƒ…æ„Ÿææ€§åˆ†æ")
        
        return "default"


class AsyncSentimentAttributeAnalysisBatchNode(AsyncParallelBatchNode):
    """
    å¼‚æ­¥æƒ…æ„Ÿå±æ€§åˆ†æèŠ‚ç‚¹
    
    åŠŸèƒ½ï¼šæ‰¹é‡åˆ†æåšæ–‡çš„å…·ä½“æƒ…æ„ŸçŠ¶æ€
    ç±»å‹ï¼šAsyncParallelBatchNode
    è¾“å‡ºå­—æ®µï¼šsentiment_attribute (List[str])
    
    ä»é¢„å®šä¹‰æƒ…æ„Ÿå±æ€§åˆ—è¡¨ä¸­é€‰æ‹©1-3ä¸ªæœ€è´´åˆ‡çš„å±æ€§
    """
    
    async def prep_async(self, shared):
        """è¿”å›åšæ–‡å’Œæƒ…æ„Ÿå±æ€§çš„ç»„åˆåˆ—è¡¨"""
        blog_data = shared.get("data", {}).get("blog_data", [])
        sentiment_attributes = shared.get("data", {}).get("sentiment_attributes", [])
        
        return [{
            "blog_data": blog_post,
            "sentiment_attributes": sentiment_attributes
        } for blog_post in blog_data]
    
    async def exec_async(self, prep_res):
        """å¯¹å•æ¡åšæ–‡è°ƒç”¨LLMè¿›è¡Œæƒ…æ„Ÿå±æ€§åˆ†æ"""
        blog_post = prep_res["blog_data"]
        sentiment_attributes = prep_res["sentiment_attributes"]
        
        attributes_str = "ã€".join(sentiment_attributes)
        
        prompt = f"""ä»å€™é€‰æƒ…æ„Ÿå±æ€§ä¸­é€‰å‡ºæœ€è´´åˆ‡çš„1-3ä¸ªï¼ŒæŒ‰JSONæ•°ç»„è¾“å‡ºï¼ˆç¤ºä¾‹ï¼š["æ”¯æŒ","æœŸå¾…"]ï¼‰ã€‚
å€™é€‰ï¼š{attributes_str}
ä»…è¾“å‡ºJSONæ•°ç»„ï¼Œä¸å¾—æ·»åŠ è§£é‡Šæˆ–å…¶ä»–æ–‡æœ¬ã€‚
åšæ–‡ï¼š
{blog_post.get('content', '')}"""
        
        response = await asyncio.to_thread(call_glm_45_air, prompt, temperature=0.3)
        
        attributes = json.loads(response.strip())
        if not isinstance(attributes, list):
            raise ValueError(f"æ¨¡å‹è¾“å‡ºä¸æ˜¯åˆ—è¡¨æ ¼å¼: {attributes}")
        
        # éªŒè¯å¹¶è¿‡æ»¤æœ‰æ•ˆå±æ€§
        return [attr for attr in attributes if attr in sentiment_attributes]
    
    async def exec_fallback_async(self, prep_res, exc):
        """åˆ†æå¤±è´¥æ—¶è¿”å›ä¸­ç«‹å±æ€§"""
        print(f"æƒ…æ„Ÿå±æ€§åˆ†æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼: {str(exc)}")
        return ["ä¸­ç«‹"]
    
    async def post_async(self, shared, prep_res, exec_res):
        """å°†åˆ†æç»“æœé™„åŠ åˆ°åšæ–‡å¯¹è±¡"""
        blog_data = shared.get("data", {}).get("blog_data", [])
        
        if len(exec_res) != len(blog_data):
            print("è­¦å‘Šï¼šæƒ…æ„Ÿå±æ€§åˆ†æç»“æœæ•°é‡ä¸åšæ–‡æ•°é‡ä¸åŒ¹é…")
            return "default"
        
        for i, blog_post in enumerate(blog_data):
            blog_post['sentiment_attribute'] = exec_res[i] if i < len(exec_res) else None
        
        print(f"[AsyncSentimentAttribute] å®Œæˆ {len(exec_res)} æ¡åšæ–‡çš„æƒ…æ„Ÿå±æ€§åˆ†æ")
        
        return "default"


class AsyncTwoLevelTopicAnalysisBatchNode(AsyncParallelBatchNode):
    """
    å¼‚æ­¥ä¸¤çº§ä¸»é¢˜åˆ†æèŠ‚ç‚¹
    
    åŠŸèƒ½ï¼šæ‰¹é‡ä»é¢„å®šä¹‰ä¸»é¢˜åˆ—è¡¨ä¸­é€‰æ‹©åˆé€‚ä¸»é¢˜
    ç±»å‹ï¼šAsyncParallelBatchNode
    è¾“å‡ºå­—æ®µï¼štopics (List[Dict])
    
    ä»é¢„å®šä¹‰çš„ä¸¤å±‚åµŒå¥—ä¸»é¢˜åˆ—è¡¨ä¸­é€‰æ‹©1-2ä¸ªçˆ¶/å­ä¸»é¢˜ç»„åˆ
    """
    
    async def prep_async(self, shared):
        """è¿”å›åšæ–‡å’Œä¸»é¢˜å±‚æ¬¡ç»“æ„çš„ç»„åˆåˆ—è¡¨"""
        blog_data = shared.get("data", {}).get("blog_data", [])
        topics_hierarchy = shared.get("data", {}).get("topics_hierarchy", [])
        
        return [{
            "blog_data": blog_post,
            "topics_hierarchy": topics_hierarchy
        } for blog_post in blog_data]
    
    async def exec_async(self, prep_res):
        """å¯¹å•æ¡åšæ–‡è°ƒç”¨å¤šæ¨¡æ€LLMè¿›è¡Œä¸»é¢˜åŒ¹é…"""
        blog_post = prep_res["blog_data"]
        topics_hierarchy = prep_res["topics_hierarchy"]
        
        # æ„å»ºä¸»é¢˜å±‚æ¬¡ç»“æ„å­—ç¬¦ä¸²
        topics_lines = []
        for topic_group in topics_hierarchy:
            parent_topic = topic_group.get("parent_topic", "")
            sub_topics = "ã€".join(topic_group.get("sub_topics", []))
            topics_lines.append(f"{parent_topic} -> {sub_topics}")
        topics_str = "\n".join(topics_lines)
        
        prompt = f"""è¯·æ ¹æ®ä»¥ä¸‹ä¸»é¢˜å±‚æ¬¡ï¼Œä»å€™é€‰ä¸­é€‰1-2ä¸ªæœ€è´´åˆ‡çš„çˆ¶/å­ä¸»é¢˜ç»„åˆï¼Œä½¿ç”¨JSONæ•°ç»„è¾“å‡ºï¼š
[{{"parent_topic": "çˆ¶ä¸»é¢˜", "sub_topic": "å­ä¸»é¢˜"}}]
è‹¥æ— åŒ¹é…è¾“å‡º []ï¼Œä¸å¾—æ·»åŠ è§£é‡Šã€‚
å€™é€‰ä¸»é¢˜ï¼š
{topics_str}
åšæ–‡ï¼š
{blog_post.get('content', '')}"""
        
        # å¤„ç†å›¾ç‰‡è·¯å¾„
        image_paths = blog_post.get('image_urls', [])
        image_paths = [img for img in image_paths if img and img.strip()]
        
        processed_image_paths = []
        for img_path in image_paths:
            if not os.path.isabs(img_path):
                processed_image_paths.append(os.path.join("data", img_path))
            else:
                processed_image_paths.append(img_path)
        
        # å¼‚æ­¥è°ƒç”¨LLM
        if processed_image_paths:
            response = await asyncio.to_thread(
                call_glm4v_plus, prompt, image_paths=processed_image_paths, temperature=0.3
            )
        else:
            response = await asyncio.to_thread(
                call_glm_45_air, prompt, temperature=0.3
            )
        
        topics = json.loads(response.strip())
        if not isinstance(topics, list):
            raise ValueError(f"æ¨¡å‹è¾“å‡ºä¸æ˜¯åˆ—è¡¨æ ¼å¼: {topics}")
        
        # éªŒè¯å¹¶è¿‡æ»¤æœ‰æ•ˆä¸»é¢˜
        valid_topics = []
        for topic_item in topics:
            parent_topic = topic_item.get("parent_topic", "")
            sub_topic = topic_item.get("sub_topic", "")
            
            for topic_group in topics_hierarchy:
                if topic_group.get("parent_topic") == parent_topic:
                    if sub_topic in topic_group.get("sub_topics", []):
                        valid_topics.append({"parent_topic": parent_topic, "sub_topic": sub_topic})
                    break
        
        return valid_topics
    
    async def exec_fallback_async(self, prep_res, exc):
        """åˆ†æå¤±è´¥æ—¶è¿”å›ç©ºåˆ—è¡¨"""
        print(f"ä¸»é¢˜åˆ†æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼: {str(exc)}")
        return []
    
    async def post_async(self, shared, prep_res, exec_res):
        """å°†åˆ†æç»“æœé™„åŠ åˆ°åšæ–‡å¯¹è±¡"""
        blog_data = shared.get("data", {}).get("blog_data", [])
        
        if len(exec_res) != len(blog_data):
            print("è­¦å‘Šï¼šä¸»é¢˜åˆ†æç»“æœæ•°é‡ä¸åšæ–‡æ•°é‡ä¸åŒ¹é…")
            return "default"
        
        for i, blog_post in enumerate(blog_data):
            blog_post['topics'] = exec_res[i] if i < len(exec_res) else None
        
        print(f"[AsyncTopic] å®Œæˆ {len(exec_res)} æ¡åšæ–‡çš„ä¸»é¢˜åˆ†æ")
        
        return "default"


class AsyncPublisherObjectAnalysisBatchNode(AsyncParallelBatchNode):
    """
    å¼‚æ­¥å‘å¸ƒè€…å¯¹è±¡åˆ†æèŠ‚ç‚¹
    
    åŠŸèƒ½ï¼šæ‰¹é‡è¯†åˆ«å‘å¸ƒè€…ç±»å‹
    ç±»å‹ï¼šAsyncParallelBatchNode
    è¾“å‡ºå­—æ®µï¼špublisher (str)
    
    ä»é¢„å®šä¹‰å‘å¸ƒè€…ç±»å‹åˆ—è¡¨ä¸­é€‰æ‹©ä¸€ä¸ªæœ€åŒ¹é…çš„ç±»å‹ï¼š
    æ”¿åºœæœºæ„ã€å®˜æ–¹æ–°é—»åª’ä½“ã€è‡ªåª’ä½“ã€ä¼ä¸šè´¦å·ã€ä¸ªäººç”¨æˆ·ç­‰
    """
    
    async def prep_async(self, shared):
        """è¿”å›åšæ–‡å’Œå‘å¸ƒè€…ç±»å‹çš„ç»„åˆåˆ—è¡¨"""
        blog_data = shared.get("data", {}).get("blog_data", [])
        publisher_objects = shared.get("data", {}).get("publisher_objects", [])
        
        return [{
            "blog_data": blog_post,
            "publisher_objects": publisher_objects
        } for blog_post in blog_data]
    
    async def exec_async(self, prep_res):
        """å¯¹å•æ¡åšæ–‡è°ƒç”¨LLMè¿›è¡Œå‘å¸ƒè€…ç±»å‹è¯†åˆ«"""
        blog_post = prep_res["blog_data"]
        publisher_objects = prep_res["publisher_objects"]
        
        publishers_str = "ã€".join(publisher_objects)
        
        prompt = f"""å€™é€‰å‘å¸ƒè€…ï¼š{publishers_str}
åˆ¤æ–­è¯¥åšæ–‡æœ€å¯èƒ½çš„å‘å¸ƒè€…ç±»å‹ï¼Œç›´æ¥è¾“å‡ºå€™é€‰åˆ—è¡¨ä¸­çš„ä¸€ä¸ªæ¡ç›®ï¼Œä¸å¾—æ·»åŠ è§£é‡Šã€‚
åšæ–‡ï¼š
{blog_post.get('content', '')}"""
        
        response = await asyncio.to_thread(call_glm_45_air, prompt, temperature=0.3)
        
        publisher = response.strip()
        
        if publisher in publisher_objects:
            return publisher
        else:
            return "ä¸ªäººç”¨æˆ·" if "ä¸ªäººç”¨æˆ·" in publisher_objects else None
    
    async def exec_fallback_async(self, prep_res, exc):
        """åˆ†æå¤±è´¥æ—¶è¿”å›ä¸ªäººç”¨æˆ·"""
        print(f"å‘å¸ƒè€…åˆ†æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼: {str(exc)}")
        return "ä¸ªäººç”¨æˆ·"
    
    async def post_async(self, shared, prep_res, exec_res):
        """å°†åˆ†æç»“æœé™„åŠ åˆ°åšæ–‡å¯¹è±¡"""
        blog_data = shared.get("data", {}).get("blog_data", [])
        
        if len(exec_res) != len(blog_data):
            print("è­¦å‘Šï¼šå‘å¸ƒè€…åˆ†æç»“æœæ•°é‡ä¸åšæ–‡æ•°é‡ä¸åŒ¹é…")
            return "default"
        
        for i, blog_post in enumerate(blog_data):
            blog_post['publisher'] = exec_res[i] if i < len(exec_res) else None
        
        print(f"[AsyncPublisher] å®Œæˆ {len(exec_res)} æ¡åšæ–‡çš„å‘å¸ƒè€…åˆ†æ")
        
        return "default"


# -----------------------------------------------------------------------------
# 3.3 Batch APIè·¯å¾„èŠ‚ç‚¹ (enhancement_mode="batch_api")
# -----------------------------------------------------------------------------

class BatchAPIEnhancementNode(Node):
    """
    Batch APIå¢å¼ºå¤„ç†èŠ‚ç‚¹
    
    åŠŸèƒ½ï¼šè°ƒç”¨batch/ç›®å½•ä¸‹çš„è„šæœ¬è¿›è¡Œæ‰¹é‡å¤„ç†
    ç±»å‹ï¼šRegular Node
    
    å¤„ç†æµç¨‹ï¼š
    1. è°ƒç”¨ batch/batch_run.py è„šæœ¬æ‰§è¡Œå®Œæ•´çš„Batch APIæµç¨‹
    2. ç­‰å¾…å¤„ç†å®Œæˆ
    3. åŠ è½½å¤„ç†ç»“æœåˆ°sharedä¸­
    
    Batch APIæµç¨‹åŒ…æ‹¬ï¼š
    - generate_jsonl.py: ç”Ÿæˆæ‰¹é‡è¯·æ±‚æ–‡ä»¶
    - upload_and_start.py: ä¸Šä¼ å¹¶å¯åŠ¨ä»»åŠ¡
    - download_results.py: ä¸‹è½½ç»“æœ
    - parse_and_integrate.py: è§£æå¹¶æ•´åˆç»“æœ
    """
    
    def prep(self, shared):
        """è¯»å–é…ç½®å‚æ•°"""
        config = shared.get("config", {})
        batch_config = config.get("batch_api_config", {})
        
        return {
            "batch_script_path": batch_config.get("script_path", "batch/batch_run.py"),
            "input_data_path": batch_config.get("input_path", "data/beijing_rainstorm_posts.json"),
            "output_data_path": batch_config.get("output_path", "data/enhanced_blogs.json"),
            "wait_for_completion": batch_config.get("wait_for_completion", True)
        }
    
    def exec(self, prep_res):
        """æ‰§è¡ŒBatch APIå¤„ç†è„šæœ¬"""
        batch_script_path = prep_res["batch_script_path"]
        
        print(f"\n[BatchAPI] å¼€å§‹æ‰§è¡ŒBatch APIå¤„ç†...")
        print(f"[BatchAPI] è„šæœ¬è·¯å¾„: {batch_script_path}")
        
        # æ£€æŸ¥è„šæœ¬æ˜¯å¦å­˜åœ¨
        if not os.path.exists(batch_script_path):
            return {
                "success": False,
                "error": f"Batchè„šæœ¬ä¸å­˜åœ¨: {batch_script_path}",
                "output_path": prep_res["output_data_path"]
            }
        
        try:
            # æ‰§è¡Œbatch_run.pyè„šæœ¬
            result = subprocess.run(
                ["python", batch_script_path],
                capture_output=True,
                text=True,
                cwd=os.getcwd()
            )
            
            if result.returncode == 0:
                print(f"[BatchAPI] è„šæœ¬æ‰§è¡ŒæˆåŠŸ")
                print(result.stdout)
                return {
                    "success": True,
                    "output_path": prep_res["output_data_path"],
                    "stdout": result.stdout
                }
            else:
                print(f"[BatchAPI] è„šæœ¬æ‰§è¡Œå¤±è´¥")
                print(f"é”™è¯¯è¾“å‡º: {result.stderr}")
                return {
                    "success": False,
                    "error": result.stderr,
                    "output_path": prep_res["output_data_path"]
                }
                
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "output_path": prep_res["output_data_path"]
            }
    
    def post(self, shared, prep_res, exec_res):
        """åŠ è½½å¤„ç†ç»“æœï¼Œæ›´æ–°shared"""
        if exec_res["success"]:
            # å°è¯•åŠ è½½å¤„ç†åçš„æ•°æ®
            output_path = exec_res["output_path"]
            
            if os.path.exists(output_path):
                try:
                    enhanced_data = load_enhanced_blog_data(output_path)
                    shared["data"]["blog_data"] = enhanced_data
                    
                    print(f"[BatchAPI] âœ“ æˆåŠŸåŠ è½½ {len(enhanced_data)} æ¡å¢å¼ºæ•°æ®")
                    
                    if "stage1_results" not in shared:
                        shared["stage1_results"] = {}
                    shared["stage1_results"]["batch_api"] = {
                        "success": True,
                        "data_count": len(enhanced_data)
                    }
                except Exception as e:
                    print(f"[BatchAPI] âœ— åŠ è½½å¢å¼ºæ•°æ®å¤±è´¥: {str(e)}")
                    shared["stage1_results"]["batch_api"] = {
                        "success": False,
                        "error": str(e)
                    }
            else:
                print(f"[BatchAPI] âœ— è¾“å‡ºæ–‡ä»¶ä¸å­˜åœ¨: {output_path}")
                shared["stage1_results"]["batch_api"] = {
                    "success": False,
                    "error": f"è¾“å‡ºæ–‡ä»¶ä¸å­˜åœ¨: {output_path}"
                }
        else:
            print(f"[BatchAPI] âœ— Batch APIå¤„ç†å¤±è´¥: {exec_res.get('error', 'Unknown error')}")
            if "stage1_results" not in shared:
                shared["stage1_results"] = {}
            shared["stage1_results"]["batch_api"] = {
                "success": False,
                "error": exec_res.get("error", "Unknown error")
            }
        
        return "default"


# =============================================================================
# 4. é˜¶æ®µ2èŠ‚ç‚¹: åˆ†ææ‰§è¡Œï¼ˆå¾…å®ç°ï¼‰
# =============================================================================

# TODO: å®ç°ä»¥ä¸‹èŠ‚ç‚¹
# - Stage2EntryNode: é˜¶æ®µ2å…¥å£èŠ‚ç‚¹
# - WorkflowAnalysisNode: å›ºå®šè„šæœ¬åˆ†æèŠ‚ç‚¹
# - CollectToolsNode: å·¥å…·æ”¶é›†èŠ‚ç‚¹
# - DecisionToolsNode: å·¥å…·å†³ç­–èŠ‚ç‚¹
# - ExecuteToolsNode: å·¥å…·æ‰§è¡ŒèŠ‚ç‚¹
# - ProcessResultNode: ç»“æœå¤„ç†èŠ‚ç‚¹
# - Stage2CompletionNode: é˜¶æ®µ2å®ŒæˆèŠ‚ç‚¹


# =============================================================================
# 5. é˜¶æ®µ3èŠ‚ç‚¹: æŠ¥å‘Šç”Ÿæˆï¼ˆå¾…å®ç°ï¼‰
# =============================================================================

# TODO: å®ç°ä»¥ä¸‹èŠ‚ç‚¹
# - Stage3EntryNode: é˜¶æ®µ3å…¥å£èŠ‚ç‚¹
# - TemplateReportNode: æ¨¡æ¿å¡«å……æŠ¥å‘ŠèŠ‚ç‚¹
# - GenerateReportNode: æŠ¥å‘Šç”ŸæˆèŠ‚ç‚¹
# - ReviewReportNode: æŠ¥å‘Šè¯„å®¡èŠ‚ç‚¹
# - ApplyFeedbackNode: åº”ç”¨ä¿®æ”¹æ„è§èŠ‚ç‚¹
# - Stage3CompletionNode: é˜¶æ®µ3å®ŒæˆèŠ‚ç‚¹
