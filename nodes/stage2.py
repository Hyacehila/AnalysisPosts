"""
stage2.py - é˜¶æ®µ2èŠ‚ç‚¹ï¼šåˆ†ææ‰§è¡Œ

åŒ…å«æ•°æ®åŠ è½½/æ‘˜è¦/ä¿å­˜èŠ‚ç‚¹ + Workflowè·¯å¾„èŠ‚ç‚¹ + Agentå¾ªç¯èŠ‚ç‚¹ã€‚
"""

import json
import os
import asyncio
import subprocess
import time
import re
from pathlib import Path
from typing import Any, Dict, List, Optional
from collections import Counter, defaultdict
from datetime import datetime

from pocketflow import Node, AsyncNode

from nodes._utils import normalize_path, get_project_relative_path, ensure_dir_exists
from utils.call_llm import call_glm_45_air, call_glm4v_plus, call_glm45v_thinking, call_glm46
from utils.data_loader import load_enhanced_blog_data





# =============================================================================
# 4. é˜¶æ®µ2èŠ‚ç‚¹: åˆ†ææ‰§è¡Œ
# =============================================================================

# -----------------------------------------------------------------------------
# 4.1 é€šç”¨èŠ‚ç‚¹
# -----------------------------------------------------------------------------

class LoadEnhancedDataNode(Node):
    """
    åŠ è½½å¢å¼ºæ•°æ®èŠ‚ç‚¹
    
    åŠŸèƒ½ï¼šåŠ è½½å·²å®Œæˆå¢å¼ºå¤„ç†çš„åšæ–‡æ•°æ®
    ç±»å‹ï¼šRegular Node
    å‰ç½®æ£€æŸ¥ï¼šéªŒè¯é˜¶æ®µ1è¾“å‡ºæ–‡ä»¶æ˜¯å¦å­˜åœ¨
    """
    
    def prep(self, shared):
        """è¯»å–å¢å¼ºæ•°æ®æ–‡ä»¶è·¯å¾„ï¼Œæ£€æŸ¥å‰ç½®æ¡ä»¶"""
        config = shared.get("config", {})
        enhanced_data_path = config.get("data_source", {}).get(
            "enhanced_data_path", "data/enhanced_blogs.json"
        )
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(enhanced_data_path):
            raise FileNotFoundError(
                f"é˜¶æ®µ1è¾“å‡ºæ–‡ä»¶ä¸å­˜åœ¨: {enhanced_data_path}\n"
                f"è¯·å…ˆè¿è¡Œé˜¶æ®µ1ï¼ˆå¢å¼ºå¤„ç†ï¼‰æˆ–ç¡®ä¿æ–‡ä»¶è·¯å¾„æ­£ç¡®"
            )
        
        return {"data_path": enhanced_data_path}
    
    def exec(self, prep_res):
        """åŠ è½½JSONæ•°æ®ï¼ŒéªŒè¯å¢å¼ºå­—æ®µå®Œæ•´æ€§"""
        data_path = prep_res["data_path"]
        
        print(f"\n[LoadEnhancedData] åŠ è½½å¢å¼ºæ•°æ®: {data_path}")
        blog_data = load_enhanced_blog_data(data_path)
        
        # éªŒè¯å¢å¼ºå­—æ®µ
        enhanced_fields = ["sentiment_polarity", "sentiment_attribute", "topics", "publisher"]
        valid_count = 0
        for post in blog_data:
            has_all_fields = all(post.get(field) is not None for field in enhanced_fields)
            if has_all_fields:
                valid_count += 1
        
        return {
            "blog_data": blog_data,
            "total_count": len(blog_data),
            "valid_count": valid_count,
            "enhancement_rate": round(valid_count / len(blog_data) * 100, 2) if blog_data else 0
        }
    
    def post(self, shared, prep_res, exec_res):
        """å­˜å‚¨æ•°æ®åˆ°shared"""
        if "data" not in shared:
            shared["data"] = {}
        
        shared["data"]["blog_data"] = exec_res["blog_data"]
        
        print(f"[LoadEnhancedData] [âˆš] åŠ è½½ {exec_res['total_count']} æ¡åšæ–‡")
        print(f"[LoadEnhancedData] [âˆš] å®Œæ•´å¢å¼ºç‡: {exec_res['enhancement_rate']}%")
        
        return "default"


class DataSummaryNode(Node):
    """
    æ•°æ®æ¦‚å†µç”ŸæˆèŠ‚ç‚¹
    
    åŠŸèƒ½ï¼šç”Ÿæˆå¢å¼ºæ•°æ®çš„ç»Ÿè®¡æ¦‚å†µï¼ˆä¾›Agentå†³ç­–å‚è€ƒï¼‰
    ç±»å‹ï¼šRegular Node
    """
    
    def prep(self, shared):
        """è¯»å–å¢å¼ºæ•°æ®"""
        return shared.get("data", {}).get("blog_data", [])
    
    def exec(self, prep_res):
        """è®¡ç®—å„ç»´åº¦åˆ†å¸ƒã€æ—¶é—´è·¨åº¦ã€æ€»é‡ç­‰ç»Ÿè®¡ä¿¡æ¯"""
        blog_data = prep_res
        
        if not blog_data:
            return {"summary": "æ— æ•°æ®", "statistics": {}}
        
        from collections import Counter
        from datetime import datetime
        
        # åŸºç¡€ç»Ÿè®¡
        total = len(blog_data)
        
        # æƒ…æ„Ÿåˆ†å¸ƒ
        sentiment_dist = Counter(p.get("sentiment_polarity") for p in blog_data if p.get("sentiment_polarity"))
        
        # å‘å¸ƒè€…åˆ†å¸ƒ
        publisher_dist = Counter(p.get("publisher") for p in blog_data if p.get("publisher"))
        
        # ä¸»é¢˜åˆ†å¸ƒ
        parent_topics = Counter()
        for p in blog_data:
            topics = p.get("topics") or []
            if not isinstance(topics, list):
                continue
            for t in topics:
                if isinstance(t, dict) and t.get("parent_topic"):
                    parent_topics[t["parent_topic"]] += 1
        
        # åœ°ç†åˆ†å¸ƒ
        location_dist = Counter(p.get("location") for p in blog_data if p.get("location"))
        
        # æ—¶é—´èŒƒå›´
        publish_times = []
        for p in blog_data:
            pt = p.get("publish_time")
            if pt:
                try:
                    publish_times.append(datetime.strptime(pt, "%Y-%m-%d %H:%M:%S"))
                except:
                    pass
        
        time_range = None
        if publish_times:
            time_range = {
                "start": min(publish_times).strftime("%Y-%m-%d %H:%M:%S"),
                "end": max(publish_times).strftime("%Y-%m-%d %H:%M:%S"),
                "span_hours": round((max(publish_times) - min(publish_times)).total_seconds() / 3600, 1)
            }
        
        # äº’åŠ¨ç»Ÿè®¡
        total_reposts = sum(p.get("repost_count", 0) for p in blog_data)
        total_comments = sum(p.get("comment_count", 0) for p in blog_data)
        total_likes = sum(p.get("like_count", 0) for p in blog_data)
        
        summary_text = f"""æ•°æ®æ¦‚å†µ:
- æ€»åšæ–‡æ•°: {total}
- æ—¶é—´èŒƒå›´: {time_range['start'] if time_range else 'æœªçŸ¥'} è‡³ {time_range['end'] if time_range else 'æœªçŸ¥'}
- æƒ…æ„Ÿåˆ†å¸ƒ: {dict(sentiment_dist.most_common(5))}
- çƒ­é—¨ä¸»é¢˜Top3: {[t[0] for t in parent_topics.most_common(3)]}
- ä¸»è¦åœ°åŒºTop3: {[l[0] for l in location_dist.most_common(3)]}
- å‘å¸ƒè€…ç±»å‹: {list(publisher_dist.keys())}
- æ€»äº’åŠ¨é‡: è½¬å‘{total_reposts}, è¯„è®º{total_comments}, ç‚¹èµ{total_likes}"""
        
        return {
            "summary": summary_text,
            "statistics": {
                "total_posts": total,
                "time_range": time_range,
                "sentiment_distribution": dict(sentiment_dist),
                "publisher_distribution": dict(publisher_dist),
                "topic_distribution": dict(parent_topics.most_common(10)),
                "location_distribution": dict(location_dist.most_common(10)),
                "engagement": {
                    "total_reposts": total_reposts,
                    "total_comments": total_comments,
                    "total_likes": total_likes
                }
            }
        }
    
    def post(self, shared, prep_res, exec_res):
        """å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯"""
        if "agent" not in shared:
            shared["agent"] = {}
        
        shared["agent"]["data_summary"] = exec_res["summary"]
        shared["agent"]["data_statistics"] = exec_res["statistics"]
        
        print(f"\n[DataSummary] æ•°æ®æ¦‚å†µå·²ç”Ÿæˆ")
        print(exec_res["summary"])
        
        return "default"


class SaveAnalysisResultsNode(Node):
    """
    ä¿å­˜åˆ†æç»“æœèŠ‚ç‚¹

    åŠŸèƒ½ï¼šå°†åˆ†æç»“æœæŒä¹…åŒ–ï¼Œä¾›é˜¶æ®µ3ä½¿ç”¨
    ç±»å‹ï¼šRegular Node
    è¾“å‡ºä½ç½®ï¼š
    - ç»Ÿè®¡æ•°æ®ï¼šreport/analysis_data.json
    - å›¾è¡¨åˆ†æï¼šreport/chart_analyses.json
    - æ´å¯Ÿæè¿°ï¼šreport/insights.json
    - å›¾è¡¨æ–‡ä»¶ï¼šreport/images/
    """

    def prep(self, shared):
        """è¯»å–åˆ†æè¾“å‡ºã€å›¾è¡¨åˆ—è¡¨å’Œå›¾è¡¨åˆ†æç»“æœ"""
        stage2_results = shared.get("stage2_results", {})

        return {
            "charts": stage2_results.get("charts", []),
            "tables": stage2_results.get("tables", []),
            "chart_analyses": stage2_results.get("chart_analyses", {}),
            "insights": stage2_results.get("insights", {}),
            "execution_log": stage2_results.get("execution_log", {})
        }
    
    def exec(self, prep_res):
        """ä¿å­˜JSONç»“æœæ–‡ä»¶"""
        output_dir = "report"
        os.makedirs(output_dir, exist_ok=True)

        # ä¿å­˜åˆ†ææ•°æ®
        analysis_data = {
            "charts": prep_res["charts"],
            "tables": prep_res["tables"],
            "execution_log": prep_res["execution_log"]
        }

        analysis_data_path = os.path.join(output_dir, "analysis_data.json")
        with open(analysis_data_path, 'w', encoding='utf-8') as f:
            json.dump(analysis_data, f, ensure_ascii=False, indent=2)

        # ä¿å­˜å›¾è¡¨åˆ†æç»“æœï¼ˆæ–°å¢ï¼‰
        chart_analyses_path = os.path.join(output_dir, "chart_analyses.json")
        with open(chart_analyses_path, 'w', encoding='utf-8') as f:
            json.dump(prep_res["chart_analyses"], f, ensure_ascii=False, indent=2)

        # ä¿å­˜æ´å¯Ÿæè¿°
        insights_path = os.path.join(output_dir, "insights.json")
        with open(insights_path, 'w', encoding='utf-8') as f:
            json.dump(prep_res["insights"], f, ensure_ascii=False, indent=2)

        return {
            "success": True,
            "analysis_data_path": analysis_data_path,
            "chart_analyses_path": chart_analyses_path,
            "insights_path": insights_path,
            "charts_count": len(prep_res["charts"]),
            "tables_count": len(prep_res["tables"]),
            "chart_analyses_count": len(prep_res["chart_analyses"])
        }
    
    def post(self, shared, prep_res, exec_res):
        """è®°å½•ä¿å­˜çŠ¶æ€"""
        if "stage2_results" not in shared:
            shared["stage2_results"] = {}

        shared["stage2_results"]["output_files"] = {
            "charts_dir": "report/images/",
            "analysis_data": exec_res["analysis_data_path"],
            "chart_analyses_file": exec_res["chart_analyses_path"],
            "insights_file": exec_res["insights_path"]
        }

        print(f"\n[SaveAnalysisResults] [OK] åˆ†æç»“æœå·²ä¿å­˜")
        print(f"  - åˆ†ææ•°æ®: {exec_res['analysis_data_path']}")
        print(f"  - å›¾è¡¨åˆ†æ: {exec_res['chart_analyses_path']}")
        print(f"  - æ´å¯Ÿæè¿°: {exec_res['insights_path']}")
        print(f"  - ç”Ÿæˆå›¾è¡¨: {exec_res['charts_count']} ä¸ª")
        print(f"  - åˆ†æå›¾è¡¨: {exec_res['chart_analyses_count']} ä¸ª")
        print(f"  - ç”Ÿæˆè¡¨æ ¼: {exec_res['tables_count']} ä¸ª")

        return "default"




# -----------------------------------------------------------------------------
# 4.2 é¢„å®šä¹‰Workflowè·¯å¾„èŠ‚ç‚¹ (analysis_mode="workflow")
# -----------------------------------------------------------------------------

class ExecuteAnalysisScriptNode(Node):
    """
    æ‰§è¡Œåˆ†æè„šæœ¬èŠ‚ç‚¹
    
    åŠŸèƒ½ï¼šæ‰§è¡Œå›ºå®šçš„åˆ†æè„šæœ¬ï¼Œç”Ÿæˆå…¨éƒ¨æ‰€éœ€å›¾å½¢
    ç±»å‹ï¼šRegular Node
    
    æ‰§è¡Œå››ç±»å·¥å…·é›†çš„å…¨éƒ¨å·¥å…·å‡½æ•°ï¼š
    - æƒ…æ„Ÿè¶‹åŠ¿åˆ†æå·¥å…·é›†
    - ä¸»é¢˜æ¼”åŒ–åˆ†æå·¥å…·é›†
    - åœ°ç†åˆ†å¸ƒåˆ†æå·¥å…·é›†
    - å¤šç»´äº¤äº’åˆ†æå·¥å…·é›†
    """
    
    def prep(self, shared):
        """è¯»å–å¢å¼ºæ•°æ®"""
        return shared.get("data", {}).get("blog_data", [])
    
    def exec(self, prep_res):
        """æ‰§è¡Œé¢„å®šä¹‰çš„åˆ†æè„šæœ¬"""
        from utils.analysis_tools import (
            # æƒ…æ„Ÿå·¥å…·
            sentiment_distribution_stats,
            sentiment_time_series,
            sentiment_anomaly_detection,
            sentiment_trend_chart,
            sentiment_pie_chart,
            sentiment_bucket_trend_chart,
            sentiment_attribute_trend_chart,
            sentiment_focus_window_chart,
            sentiment_focus_publisher_chart,
            # ä¸»é¢˜å·¥å…·
            topic_frequency_stats,
            topic_time_evolution,
            topic_cooccurrence_analysis,
            topic_ranking_chart,
            topic_evolution_chart,
            topic_network_chart,
            topic_focus_evolution_chart,
            topic_keyword_trend_chart,
            topic_focus_distribution_chart,
            # åœ°ç†å·¥å…·
            geographic_distribution_stats,
            geographic_hotspot_detection,
            geographic_sentiment_analysis,
            geographic_heatmap,
            geographic_bar_chart,
            geographic_sentiment_bar_chart,
            geographic_topic_heatmap,
            geographic_temporal_heatmap,
            # äº¤äº’å·¥å…·
            publisher_distribution_stats,
            cross_dimension_matrix,
            influence_analysis,
            correlation_analysis,
            interaction_heatmap,
            publisher_bar_chart,
            publisher_sentiment_bucket_chart,
            publisher_topic_distribution_chart,
            participant_trend_chart,
            publisher_focus_distribution_chart,
            belief_network_chart,
        )
        import time
        
        blog_data = prep_res
        start_time = time.time()
        
        charts = []
        tables = []
        tools_executed = []
        
        print("\n[ExecuteAnalysisScript] å¼€å§‹æ‰§è¡Œé¢„å®šä¹‰åˆ†æè„šæœ¬...")
        
        # === 1. æƒ…æ„Ÿè¶‹åŠ¿åˆ†æ ===
        print("\n  [CHART] æ‰§è¡Œæƒ…æ„Ÿè¶‹åŠ¿åˆ†æ...")
        
        # æƒ…æ„Ÿåˆ†å¸ƒç»Ÿè®¡
        result = sentiment_distribution_stats(blog_data)
        tables.append({
            "id": "sentiment_distribution",
            "title": "æƒ…æ„Ÿææ€§åˆ†å¸ƒç»Ÿè®¡",
            "data": result["data"],
            "source_tool": "sentiment_distribution_stats"
        })
        tools_executed.append("sentiment_distribution_stats")
        
        # æƒ…æ„Ÿæ—¶åºåˆ†æ
        sentiment_ts_result = sentiment_time_series(blog_data, granularity="hour")
        tables.append({
            "id": "sentiment_time_series",
            "title": "æƒ…æ„Ÿæ—¶åºè¶‹åŠ¿æ•°æ®",
            "data": sentiment_ts_result["data"],
            "source_tool": "sentiment_time_series"
        })
        tools_executed.append("sentiment_time_series")

        tables.append({
            "id": "sentiment_peaks",
            "title": "æƒ…æ„Ÿå³°å€¼ä¸æ‹ç‚¹",
            "data": {
                "peak_periods": sentiment_ts_result["data"].get("peak_periods", []),
                "peak_hours": sentiment_ts_result["data"].get("peak_hours", []),
                "turning_points": sentiment_ts_result["data"].get("turning_points", []),
                "volume_spikes": sentiment_ts_result["data"].get("volume_spikes", [])
            },
            "source_tool": "sentiment_time_series"
        })
        
        # æƒ…æ„Ÿå¼‚å¸¸æ£€æµ‹
        result = sentiment_anomaly_detection(blog_data)
        tables.append({
            "id": "sentiment_anomaly",
            "title": "æƒ…æ„Ÿå¼‚å¸¸ç‚¹",
            "data": result["data"],
            "source_tool": "sentiment_anomaly_detection"
        })
        tools_executed.append("sentiment_anomaly_detection")
        
        # æƒ…æ„Ÿè¶‹åŠ¿å›¾
        result = sentiment_trend_chart(blog_data)
        if result.get("charts"):
            charts.extend(result["charts"])
        tools_executed.append("sentiment_trend_chart")

        # æƒ…æ„Ÿæ¡¶è¶‹åŠ¿
        result = sentiment_bucket_trend_chart(blog_data)
        if result.get("charts"):
            charts.extend(result["charts"])
        tools_executed.append("sentiment_bucket_trend_chart")

        # æƒ…æ„Ÿå±æ€§è¶‹åŠ¿
        result = sentiment_attribute_trend_chart(blog_data, granularity="day")
        if result.get("charts"):
            charts.extend(result["charts"])
        tools_executed.append("sentiment_attribute_trend_chart")

        # ç„¦ç‚¹çª—å£æƒ…æ„Ÿè¶‹åŠ¿ï¼ˆçª—å£å†…ææ€§å‡å€¼ + ä¸‰åˆ†ç±»ï¼‰
        result = sentiment_focus_window_chart(blog_data)
        if result.get("charts"):
            charts.extend(result["charts"])
            tables.append({
                "id": "sentiment_focus_window_data",
                "title": "ç„¦ç‚¹çª—å£æƒ…æ„Ÿæ•°æ®",
                "data": result.get("data", {}),
                "source_tool": "sentiment_focus_window_chart"
            })
        tools_executed.append("sentiment_focus_window_chart")

        # ç„¦ç‚¹çª—å£å‘å¸ƒè€…æƒ…æ„Ÿè¶‹åŠ¿
        result = sentiment_focus_publisher_chart(blog_data)
        if result.get("charts"):
            charts.extend(result["charts"])
            tables.append({
                "id": "sentiment_focus_publisher_data",
                "title": "ç„¦ç‚¹çª—å£å‘å¸ƒè€…æƒ…æ„Ÿå‡å€¼",
                "data": result.get("data", {}),
                "source_tool": "sentiment_focus_publisher_chart"
            })
        tools_executed.append("sentiment_focus_publisher_chart")
        
        # æƒ…æ„Ÿé¥¼å›¾
        result = sentiment_pie_chart(blog_data)
        if result.get("charts"):
            charts.extend(result["charts"])
        tools_executed.append("sentiment_pie_chart")
        
        # === 2. ä¸»é¢˜æ¼”åŒ–åˆ†æ ===
        print("  [CHART] æ‰§è¡Œä¸»é¢˜æ¼”åŒ–åˆ†æ...")
        
        # ä¸»é¢˜é¢‘æ¬¡ç»Ÿè®¡
        result = topic_frequency_stats(blog_data)
        tables.append({
            "id": "topic_frequency",
            "title": "ä¸»é¢˜é¢‘æ¬¡ç»Ÿè®¡",
            "data": result["data"],
            "source_tool": "topic_frequency_stats"
        })
        tools_executed.append("topic_frequency_stats")
        
        # ä¸»é¢˜æ¼”åŒ–åˆ†æ
        result = topic_time_evolution(blog_data, granularity="day", top_n=5)
        tables.append({
            "id": "topic_evolution",
            "title": "ä¸»é¢˜æ¼”åŒ–æ•°æ®",
            "data": result["data"],
            "source_tool": "topic_time_evolution"
        })
        tools_executed.append("topic_time_evolution")
        
        # ä¸»é¢˜å…±ç°åˆ†æ
        result = topic_cooccurrence_analysis(blog_data)
        tables.append({
            "id": "topic_cooccurrence",
            "title": "ä¸»é¢˜å…±ç°å…³ç³»",
            "data": result["data"],
            "source_tool": "topic_cooccurrence_analysis"
        })
        tools_executed.append("topic_cooccurrence_analysis")
        
        # ä¸»é¢˜æ’è¡Œå›¾
        result = topic_ranking_chart(blog_data, top_n=10)
        if result.get("charts"):
            charts.extend(result["charts"])
        tools_executed.append("topic_ranking_chart")
        
        # ä¸»é¢˜æ¼”åŒ–å›¾
        result = topic_evolution_chart(blog_data)
        if result.get("charts"):
            charts.extend(result["charts"])
        tools_executed.append("topic_evolution_chart")

        # ä¸»é¢˜ç„¦ç‚¹æ¼”åŒ–
        result = topic_focus_evolution_chart(blog_data)
        if result.get("charts"):
            charts.extend(result["charts"])
        tools_executed.append("topic_focus_evolution_chart")

        # ç„¦ç‚¹çª—å£ä¸»é¢˜å‘å¸ƒè¶‹åŠ¿ï¼ˆç‹¬ç«‹çª—å£æ•°æ®ï¼‰
        result = topic_focus_distribution_chart(blog_data)
        if result.get("charts"):
            charts.extend(result["charts"])
            tables.append({
                "id": "topic_focus_distribution_data",
                "title": "ç„¦ç‚¹çª—å£ä¸»é¢˜å‘å¸ƒè¶‹åŠ¿æ•°æ®",
                "data": result.get("data", {}),
                "source_tool": "topic_focus_distribution_chart"
            })
        tools_executed.append("topic_focus_distribution_chart")

        # ç„¦ç‚¹å…³é”®è¯è¶‹åŠ¿
        result = topic_keyword_trend_chart(blog_data)
        if result.get("charts"):
            charts.extend(result["charts"])
        tools_executed.append("topic_keyword_trend_chart")
        
        # ä¸»é¢˜ç½‘ç»œå›¾
        result = topic_network_chart(blog_data)
        if result.get("charts"):
            charts.extend(result["charts"])
        tools_executed.append("topic_network_chart")
        
        # === 3. åœ°ç†åˆ†å¸ƒåˆ†æ ===
        print("  [CHART] æ‰§è¡Œåœ°ç†åˆ†å¸ƒåˆ†æ...")
        
        # åœ°ç†åˆ†å¸ƒç»Ÿè®¡
        result = geographic_distribution_stats(blog_data)
        tables.append({
            "id": "geographic_distribution",
            "title": "åœ°ç†åˆ†å¸ƒç»Ÿè®¡",
            "data": result["data"],
            "source_tool": "geographic_distribution_stats"
        })
        tools_executed.append("geographic_distribution_stats")
        
        # çƒ­ç‚¹åŒºåŸŸè¯†åˆ«
        result = geographic_hotspot_detection(blog_data)
        tables.append({
            "id": "geographic_hotspot",
            "title": "çƒ­ç‚¹åŒºåŸŸ",
            "data": result["data"],
            "source_tool": "geographic_hotspot_detection"
        })
        tools_executed.append("geographic_hotspot_detection")
        
        # åœ°åŒºæƒ…æ„Ÿåˆ†æ
        result = geographic_sentiment_analysis(blog_data)
        tables.append({
            "id": "geographic_sentiment",
            "title": "åœ°åŒºæƒ…æ„Ÿåˆ†æ",
            "data": result["data"],
            "source_tool": "geographic_sentiment_analysis"
        })
        tools_executed.append("geographic_sentiment_analysis")
        
        # åœ°ç†çƒ­åŠ›å›¾
        result = geographic_heatmap(blog_data)
        if result.get("charts"):
            charts.extend(result["charts"])
        tools_executed.append("geographic_heatmap")
        
        # åœ°åŒºåˆ†å¸ƒå›¾
        result = geographic_bar_chart(blog_data)
        if result.get("charts"):
            charts.extend(result["charts"])
        tools_executed.append("geographic_bar_chart")

        # åœ°åŒºæ­£è´Ÿé¢å¯¹æ¯”
        result = geographic_sentiment_bar_chart(blog_data)
        if result.get("charts"):
            charts.extend(result["charts"])
        tools_executed.append("geographic_sentiment_bar_chart")

        # åœ°åŒº Ã— ä¸»é¢˜çƒ­åŠ›å›¾
        result = geographic_topic_heatmap(blog_data)
        if result.get("charts"):
            charts.extend(result["charts"])
        tools_executed.append("geographic_topic_heatmap")

        # åœ°åŒº Ã— æ—¶é—´çƒ­åŠ›å›¾ï¼ˆå¤©ç²’åº¦ï¼‰
        result = geographic_temporal_heatmap(blog_data, granularity="day")
        if result.get("charts"):
            charts.extend(result["charts"])
        tools_executed.append("geographic_temporal_heatmap")
        
        # === 4. å¤šç»´äº¤äº’åˆ†æ ===
        print("  [CHART] æ‰§è¡Œå¤šç»´äº¤äº’åˆ†æ...")
        
        # å‘å¸ƒè€…åˆ†å¸ƒç»Ÿè®¡
        result = publisher_distribution_stats(blog_data)
        tables.append({
            "id": "publisher_distribution",
            "title": "å‘å¸ƒè€…åˆ†å¸ƒç»Ÿè®¡",
            "data": result["data"],
            "source_tool": "publisher_distribution_stats"
        })
        tools_executed.append("publisher_distribution_stats")
        
        # äº¤å‰çŸ©é˜µåˆ†æ
        result = cross_dimension_matrix(blog_data, dim1="publisher", dim2="sentiment_polarity")
        tables.append({
            "id": "cross_dimension_matrix",
            "title": "å‘å¸ƒè€…Ã—æƒ…æ„Ÿäº¤å‰çŸ©é˜µ",
            "data": result["data"],
            "source_tool": "cross_dimension_matrix"
        })
        tools_executed.append("cross_dimension_matrix")
        
        # å½±å“åŠ›åˆ†æ
        result = influence_analysis(blog_data, top_n=20)
        tables.append({
            "id": "influence_analysis",
            "title": "å½±å“åŠ›åˆ†æ",
            "data": result["data"],
            "source_tool": "influence_analysis"
        })
        tools_executed.append("influence_analysis")
        
        # ç›¸å…³æ€§åˆ†æ
        result = correlation_analysis(blog_data)
        tables.append({
            "id": "correlation_analysis",
            "title": "ç»´åº¦ç›¸å…³æ€§åˆ†æ",
            "data": result["data"],
            "source_tool": "correlation_analysis"
        })
        tools_executed.append("correlation_analysis")
        
        # äº¤äº’çƒ­åŠ›å›¾
        result = interaction_heatmap(blog_data)
        if result.get("charts"):
            charts.extend(result["charts"])
        tools_executed.append("interaction_heatmap")
        
        # å‘å¸ƒè€…åˆ†å¸ƒå›¾
        result = publisher_bar_chart(blog_data)
        if result.get("charts"):
            charts.extend(result["charts"])
        tools_executed.append("publisher_bar_chart")

        # å‘å¸ƒè€…æƒ…ç»ªæ¡¶å¯¹æ¯”
        result = publisher_sentiment_bucket_chart(blog_data)
        if result.get("charts"):
            charts.extend(result["charts"])
        tools_executed.append("publisher_sentiment_bucket_chart")

        # å‘å¸ƒè€…è¯é¢˜åå¥½
        result = publisher_topic_distribution_chart(blog_data)
        if result.get("charts"):
            charts.extend(result["charts"])
        tools_executed.append("publisher_topic_distribution_chart")

        # å‚ä¸äººæ•°è¶‹åŠ¿
        result = participant_trend_chart(blog_data, granularity="day")
        if result.get("charts"):
            charts.extend(result["charts"])
        tools_executed.append("participant_trend_chart")

        # ç„¦ç‚¹çª—å£å‘å¸ƒè€…ç±»å‹å‘å¸ƒè¶‹åŠ¿ï¼ˆç‹¬ç«‹çª—å£æ•°æ®ï¼‰
        result = publisher_focus_distribution_chart(blog_data)
        if result.get("charts"):
            charts.extend(result["charts"])
            tables.append({
                "id": "publisher_focus_distribution_data",
                "title": "ç„¦ç‚¹çª—å£å‘å¸ƒè€…ç±»å‹å‘å¸ƒè¶‹åŠ¿æ•°æ®",
                "data": result.get("data", {}),
                "source_tool": "publisher_focus_distribution_chart"
            })
        tools_executed.append("publisher_focus_distribution_chart")

        # ä¿¡å¿µç³»ç»Ÿç½‘ç»œ
        result = belief_network_chart(blog_data)
        if result.get("charts"):
            charts.extend(result["charts"])
            tables.append({
                "id": "belief_network_data",
                "title": "ä¿¡å¿µç³»ç»Ÿå…±ç°ç½‘ç»œæ•°æ®",
                "data": result.get("data", {}),
                "source_tool": "belief_network_chart"
            })
        tools_executed.append("belief_network_chart")

        # ç¡®ä¿å·²æ³¨å†Œå·¥å…·éƒ½è¢«è°ƒç”¨ï¼ˆé¿å…é—æ¼æ–°å·¥å…·ï¼‰
        try:
            from utils.analysis_tools.tool_registry import TOOL_REGISTRY
            executed_set = set(tools_executed)
            for tool_name, tool_def in TOOL_REGISTRY.items():
                if tool_name in executed_set:
                    continue
                params = {}
                for param_name, spec in (tool_def.get("parameters") or {}).items():
                    if param_name == "blog_data":
                        params[param_name] = blog_data
                    elif "default" in spec:
                        params[param_name] = spec["default"]
                result = tool_def["function"](**params)
                tools_executed.append(tool_name)
                executed_set.add(tool_name)
                if isinstance(result, dict) and result.get("charts"):
                    charts.extend(result["charts"])
                elif isinstance(result, dict) and "data" in result:
                    tables.append({
                        "id": tool_name,
                        "title": tool_def.get("description", tool_name),
                        "data": result["data"],
                        "source_tool": tool_name
                    })
        except Exception as e:
            print(f"[ExecuteAnalysisScript] [!] è‡ªåŠ¨è¡¥é½å·¥å…·å¤±è´¥: {e}")

        execution_time = time.time() - start_time
        
        print(f"\n[ExecuteAnalysisScript] [OK] åˆ†æè„šæœ¬æ‰§è¡Œå®Œæˆ")
        print(f"  - æ‰§è¡Œå·¥å…·: {len(tools_executed)} ä¸ª")
        print(f"  - ç”Ÿæˆå›¾è¡¨: {len(charts)} ä¸ª")
        print(f"  - ç”Ÿæˆè¡¨æ ¼: {len(tables)} ä¸ª")
        print(f"  - è€—æ—¶: {execution_time:.2f} ç§’")
        
        return {
            "charts": charts,
            "tables": tables,
            "tools_executed": tools_executed,
            "execution_time": execution_time
        }
    
    def post(self, shared, prep_res, exec_res):
        """å­˜å‚¨å›¾å½¢å’Œè¡¨æ ¼åˆ°shared"""
        if "stage2_results" not in shared:
            shared["stage2_results"] = {}
        
        shared["stage2_results"]["charts"] = exec_res["charts"]
        shared["stage2_results"]["tables"] = exec_res["tables"]
        shared["stage2_results"]["execution_log"] = {
            "tools_executed": exec_res["tools_executed"],
            "total_charts": len(exec_res["charts"]),
            "total_tables": len(exec_res["tables"]),
            "execution_time": exec_res["execution_time"]
        }

        return "default"


class ChartAnalysisNode(Node):
    """
    å›¾è¡¨åˆ†æèŠ‚ç‚¹ - ä½¿ç”¨GLM4.5V+æ€è€ƒæ¨¡å¼åˆ†æå›¾è¡¨

    åŠŸèƒ½ï¼šå¯¹æ¯ä¸ªç”Ÿæˆçš„å›¾è¡¨è¿›è¡Œæ·±åº¦è§†è§‰åˆ†æ
    ç±»å‹ï¼šRegular Nodeï¼ˆå…¼å®¹ç°æœ‰Workflowï¼‰

    è®¾è®¡ç‰¹ç‚¹ï¼š
    - GLM4.5V + æ€è€ƒæ¨¡å¼ï¼šæ—¢æ”¯æŒè§†è§‰ç†è§£ï¼Œåˆæ”¯æŒæ·±åº¦æ¨ç†
    - é¡ºåºå¤„ç†ï¼šä¸ºç¡®ä¿ä¸ç°æœ‰Flowå…¼å®¹ï¼Œé‡‡ç”¨åŒæ­¥å¤„ç†
    - ç»“æ„åŒ–è¾“å‡ºï¼šæä¾›ä¸€è‡´æ€§çš„åˆ†æç»“æœæ ¼å¼
    """

    def __init__(self, max_retries: int = 3, wait: int = 2):
        """
        åˆå§‹åŒ–å›¾è¡¨åˆ†æèŠ‚ç‚¹

        Args:
            max_retries: APIè°ƒç”¨å¤±è´¥é‡è¯•æ¬¡æ•°
            wait: é‡è¯•ç­‰å¾…æ—¶é—´(ç§’)
        """
        super().__init__(max_retries=max_retries, wait=wait)

    def prep(self, shared):
        """è¯»å–å›¾è¡¨åˆ—è¡¨"""
        charts = shared.get("stage2_results", {}).get("charts", [])
        limit_raw = os.getenv("CHART_ANALYSIS_LIMIT")
        if limit_raw is not None:
            try:
                limit = int(limit_raw)
                if limit < 0:
                    raise ValueError("limit must be non-negative")
                charts = charts[:limit]
                print(f"[ChartAnalysis] CHART_ANALYSIS_LIMIT={limit} applied")
            except ValueError:
                print(f"[ChartAnalysis] æ— æ•ˆçš„ CHART_ANALYSIS_LIMIT: {limit_raw}")
        print(f"\n[ChartAnalysis] å‡†å¤‡åˆ†æ {len(charts)} å¼ å›¾è¡¨")
        return charts

    def exec(self, prep_res):
        """é¡ºåºåˆ†ææ‰€æœ‰å›¾è¡¨"""
        import time
        charts = prep_res
        chart_analyses = {}
        success_count = 0

        print(f"[ChartAnalysis] å¼€å§‹é€ä¸ªåˆ†æå›¾è¡¨...")
        start_time = time.time()

        for i, chart in enumerate(charts, 1):
            chart_id = chart.get("id", f"chart_{i}")
            chart_title = chart.get("title", "")
            chart_path = (
                chart.get("path")
                or chart.get("file_path")
                or chart.get("chart_path")
                or chart.get("image_path")
                or ""
            )

            print(f"[ChartAnalysis] [{i}/{len(charts)}] åˆ†æå›¾è¡¨: {chart_title}")

            # æ„å»ºç®€åŒ–çš„åˆ†ææç¤ºè¯
            analysis_prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„èˆ†æƒ…æ•°æ®åˆ†æå¸ˆï¼Œè¯·å¯¹è¿™å¼ èˆ†æƒ…åˆ†æå›¾è¡¨è¿›è¡Œåˆ†æè¯´æ˜ã€‚

## å›¾è¡¨ä¿¡æ¯
- å›¾è¡¨ID: {chart_id}
- å›¾è¡¨æ ‡é¢˜: {chart_title}
- å›¾è¡¨ç±»å‹: {chart.get('type', 'unknown')}

## åˆ†æè¦æ±‚
è¯·åŸºäºå›¾è¡¨è§†è§‰ä¿¡æ¯æä¾›è¯¦ç»†åˆ†æï¼ŒåŒ…æ‹¬ï¼š

### å›¾è¡¨åŸºç¡€æè¿°
- å›¾è¡¨ç±»å‹å’Œç»“æ„ç‰¹å¾
- åæ ‡è½´æ ‡ç­¾å’Œåˆ»åº¦
- æ•°æ®ç³»åˆ—çš„æ ‡è¯†å’Œå›¾ä¾‹
- æ•´ä½“å¸ƒå±€å’Œè§†è§‰è®¾è®¡

### æ•°æ®ç»†èŠ‚
- æ¯ä¸ªæ•°æ®é¡¹çš„å…·ä½“æ•°å€¼
- æœ€é«˜å€¼ã€æœ€ä½å€¼åŠå…¶æ ‡è¯†
- æ•°æ®åˆ†å¸ƒç‰¹å¾å’Œè¶‹åŠ¿
- é‡è¦çš„æ•°æ®å…³ç³»

### å®è§‚æ´å¯Ÿ
- æ•°æ®åæ˜ çš„ä¸»è¦æ¨¡å¼
- è¶‹åŠ¿å˜åŒ–å’Œè½¬æŠ˜ç‚¹
- å…³é”®çš„ä¸šåŠ¡å‘ç°
- æ•°æ®è´¨é‡å’Œå¯è¯»æ€§è¯„ä¼°

è¯·ç”¨è‡ªç„¶è¯­è¨€æè¿°ï¼Œä¸è¦ä½¿ç”¨JSONæ ¼å¼ã€‚ç›´æ¥è¿”å›åˆ†æç»“æœã€‚
"""

            try:
                # è°ƒç”¨GLM4.5Våˆ†æå›¾è¡¨
                response = call_glm45v_thinking(
                    prompt=analysis_prompt,
                    image_paths=[chart_path] if chart_path and os.path.exists(chart_path) else None,
                    temperature=0.7,
                    max_tokens=2000,
                    enable_thinking=True
                )

                # ç›´æ¥ä½¿ç”¨LLMçš„è‡ªç„¶è¯­è¨€è¾“å‡ºï¼Œæ— éœ€JSONè§£æ
                analysis_result = {
                    "chart_id": chart_id,
                    "chart_title": chart_title,
                    "chart_path": chart_path,
                    "analysis_content": response.strip(),
                    "analysis_timestamp": time.time(),
                    "analysis_status": "success"
                }

                chart_analyses[chart_id] = analysis_result
                success_count += 1
                print(f"[ChartAnalysis] [âˆš] å›¾è¡¨ {chart_id} åˆ†æå®Œæˆ")
                print(f"[ChartAnalysis] [âˆš] åˆ†æé•¿åº¦: {len(response)} å­—ç¬¦")

            except Exception as e:
                # ç®€åŒ–é”™è¯¯å¤„ç†
                print(f"[ChartAnalysis] [!] å›¾è¡¨ {chart_id} åˆ†æå¤±è´¥: {str(e)}")

                # åˆ›å»ºç®€å•çš„fallbackç»“æœ
                fallback_result = {
                    "chart_id": chart_id,
                    "chart_title": chart_title,
                    "chart_path": chart_path,
                    "analysis_content": f"å›¾è¡¨åˆ†æå¤±è´¥: {str(e)}",
                    "analysis_timestamp": time.time(),
                    "analysis_status": "failed"
                }
                chart_analyses[chart_id] = fallback_result

        execution_time = time.time() - start_time

        return {
            "chart_analyses": chart_analyses,
            "success_count": success_count,
            "total_charts": len(charts),
            "success_rate": success_count/len(charts) if charts else 0,
            "execution_time": execution_time
        }

    def post(self, shared, prep_res, exec_res):
        """å­˜å‚¨åˆ†æç»“æœåˆ°shared"""
        # åˆå§‹åŒ–å›¾è¡¨åˆ†æç»“æœ
        if "stage2_results" not in shared:
            shared["stage2_results"] = {}

        # å­˜å‚¨åˆ°sharedå­—å…¸
        shared["stage2_results"]["chart_analyses"] = exec_res["chart_analyses"]

        # è¾“å‡ºæ‰§è¡Œæ‘˜è¦
        print(f"\n[ChartAnalysis] å›¾è¡¨åˆ†æå®Œæˆ:")
        print(f"  â”œâ”€ æ€»å›¾è¡¨æ•°: {exec_res['total_charts']}")
        print(f"  â”œâ”€ æˆåŠŸåˆ†æ: {exec_res['success_count']}")
        print(f"  â”œâ”€ å¤±è´¥æ•°é‡: {exec_res['total_charts'] - exec_res['success_count']}")
        print(f"  â””â”€ æˆåŠŸç‡: {exec_res['success_rate']*100:.1f}%")
        print(f"  â””â”€ è€—æ—¶: {exec_res['execution_time']:.2f}ç§’")

        # å­˜å‚¨æ‰§è¡Œæ—¥å¿—
        if "execution_log" not in shared["stage2_results"]:
            shared["stage2_results"]["execution_log"] = {}

        shared["stage2_results"]["execution_log"]["chart_analysis"] = {
            "total_charts": exec_res["total_charts"],
            "success_count": exec_res["success_count"],
            "success_rate": exec_res["success_rate"],
            "analysis_timestamp": exec_res["execution_time"]
        }

        return "default"

    

class LLMInsightNode(Node):
    """
    LLMæ´å¯Ÿè¡¥å……èŠ‚ç‚¹

    åŠŸèƒ½ï¼šåŸºäºGLM4.5Vå›¾è¡¨åˆ†æç»“æœï¼Œè°ƒç”¨LLMç”Ÿæˆç»¼åˆæ´å¯Ÿ
    ç±»å‹ï¼šRegular Node (LLM Call)

    åŸºäºå›¾è¡¨åˆ†æç»“æœå’Œç»Ÿè®¡æ•°æ®ï¼Œåˆ©ç”¨LLMç”Ÿæˆå„ç»´åº¦çš„æ·±åº¦æ´å¯Ÿæè¿°
    """

    def prep(self, shared):
        """è¯»å–å›¾è¡¨åˆ†æç»“æœå’Œç»Ÿè®¡æ•°æ®"""
        stage2_results = shared.get("stage2_results", {})

        return {
            "chart_analyses": stage2_results.get("chart_analyses", {}),
            "tables": stage2_results.get("tables", []),
            "data_summary": shared.get("agent", {}).get("data_summary", "")
        }
    
    def exec(self, prep_res):
        """åŸºäºå›¾è¡¨åˆ†æç»“æœæ„å»ºPromptè°ƒç”¨LLMï¼Œç”Ÿæˆæ·±åº¦æ´å¯Ÿ"""
        chart_analyses = prep_res["chart_analyses"]
        tables = prep_res["tables"]
        data_summary = prep_res["data_summary"]

        # æ„å»ºç®€åŒ–å›¾è¡¨åˆ†ææ‘˜è¦
        chart_summary = []
        for chart_id, analysis in chart_analyses.items():
            if analysis.get("analysis_status") == "success":
                title = analysis.get("chart_title", chart_id)
                content = analysis.get("analysis", "")

                chart_summary.append(f"### {title}")

                # æˆªå–å‰500å­—ç¬¦ä½œä¸ºæ‘˜è¦ï¼Œé¿å…è¿‡é•¿
                content_preview = content[:500] + ("..." if len(content) > 500 else "")
                chart_summary.append(content_preview)
                chart_summary.append("")
            else:
                # å¤„ç†åˆ†æå¤±è´¥çš„æƒ…å†µ
                title = analysis.get("chart_title", chart_id)
                status = analysis.get("analysis_status", "unknown")
                chart_summary.append(f"### {title}")
                chart_summary.append(f"åˆ†æçŠ¶æ€: {status}")
                chart_summary.append("")

        # æ„å»ºç»Ÿè®¡æ•°æ®æ‘˜è¦
        stats_summary = []
        for table in tables:
            title = table.get("title", "")
            data = table.get("data", {})
            summary = data.get("summary", "") if isinstance(data, dict) else ""
            if summary:
                stats_summary.append(f"- {title}: {summary}")

        # æ„å»ºå®Œæ•´æç¤ºè¯
        prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„èˆ†æƒ…æ•°æ®åˆ†æå¸ˆï¼Œè¯·ä¸¥æ ¼åŸºäºæä¾›çš„åˆ†æç»“æœï¼Œç”Ÿæˆæ•°æ®é©±åŠ¨çš„æ´å¯Ÿæ‘˜è¦ã€‚

## é‡è¦è¦æ±‚
1. **ä»…åŸºäºæä¾›çš„æ•°æ®**ï¼šæ‰€æœ‰ç»“è®ºå¿…é¡»æ¥è‡ªä¸‹é¢çš„å›¾è¡¨åˆ†æå’Œç»Ÿè®¡æ•°æ®
2. **ç¦æ­¢æ¨æµ‹**ï¼šä¸è¦å¼•å…¥å¤–éƒ¨çŸ¥è¯†æˆ–æ¨æµ‹åŸå› 
3. **æ•°æ®ç´¢å¼•**ï¼šå¼•ç”¨å…·ä½“çš„åˆ†æç»“æœä½œä¸ºæ”¯æ’‘
4. **å®¢è§‚å‡†ç¡®**ï¼šé¿å…å¤¸å¤§æˆ–ä¸»è§‚åˆ¤æ–­

## åŸºç¡€æ•°æ®
{data_summary if data_summary else "æ— åŸºç¡€æ•°æ®"}

## å›¾è¡¨åˆ†æç»“æœï¼ˆæ¥è‡ªGLM4.5Vï¼‰
{chr(10).join(chart_summary) if chart_summary else "æ— å›¾è¡¨åˆ†æç»“æœ"}

## ç»Ÿè®¡æ•°æ®
{chr(10).join(stats_summary) if stats_summary else "æ— ç»Ÿè®¡æ•°æ®"}

## åˆ†æè¦æ±‚
è¯·ä¸¥æ ¼åŸºäºä»¥ä¸Šæ•°æ®ï¼Œç”Ÿæˆä»¥ä¸‹ç»´åº¦çš„æ´å¯Ÿæ‘˜è¦ï¼š

1. **æƒ…æ„Ÿæ€åŠ¿æ€»ç»“**ï¼šåŸºäºå›¾è¡¨ä¸­çš„å…·ä½“æ•°å€¼å’Œè¶‹åŠ¿ï¼Œæ€»ç»“æƒ…æ„Ÿåˆ†å¸ƒç‰¹å¾
2. **ä¸»é¢˜åˆ†å¸ƒç‰¹å¾**ï¼šåŸºäºä¸»é¢˜å›¾è¡¨æ•°æ®ï¼Œæè¿°è¯é¢˜çƒ­åº¦åˆ†å¸ƒ
3. **åœ°åŸŸåˆ†å¸ƒç‰¹ç‚¹**ï¼šåŸºäºåœ°ç†æ•°æ®ï¼Œæ€»ç»“åŒºåŸŸåˆ†å¸ƒæ¨¡å¼
4. **å‘å¸ƒè€…è¡Œä¸ºç‰¹å¾**ï¼šåŸºäºå‘å¸ƒè€…ç±»å‹æ•°æ®ï¼Œæè¿°è¡Œä¸ºæ¨¡å¼
5. **ç»¼åˆæ•°æ®æ¦‚è§ˆ**ï¼šæ•´åˆæ‰€æœ‰æ•°æ®çš„æ•´ä½“ç‰¹å¾

## è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼JSONï¼‰
```json
{{
    "sentiment_summary": "åŸºäºå›¾è¡¨æ•°æ®æ€»ç»“çš„æƒ…æ„Ÿæ€åŠ¿",
    "topic_distribution": "åŸºäºæ•°æ®æè¿°çš„ä¸»é¢˜åˆ†å¸ƒç‰¹å¾",
    "geographic_distribution": "åŸºäºæ•°æ®çš„åœ°ç†åˆ†å¸ƒç‰¹ç‚¹",
    "publisher_behavior": "åŸºäºæ•°æ®çš„å‘å¸ƒè€…è¡Œä¸ºæ¨¡å¼",
    "overall_summary": "æ‰€æœ‰æ•°æ®çš„æ•´åˆæ€§æ€»ç»“"
}}
```

**é‡è¦**: æ¯ä¸ªæ´å¯Ÿéƒ½è¦æœ‰æ˜ç¡®çš„æ•°æ®æ”¯æ’‘ï¼Œä¸è¦æ·»åŠ æ¨æµ‹æ€§å†…å®¹ã€‚"""

        # ä¼˜å…ˆä½¿ç”¨GLM-4.6æ¨ç†æ¨¡å‹è¿›è¡Œç»¼åˆåˆ†æï¼Œå¼€å¯æ¨ç†æ¨¡å¼ä»¥è·å¾—æ›´å¥½çš„åˆ†æè´¨é‡
        # å¦‚æœGLM-4.6å¤±è´¥ï¼ˆå¦‚å¹¶å‘é™åˆ¶ï¼‰ï¼Œè‡ªåŠ¨å›é€€åˆ°GLM-4.5-air
        response = None
        use_fallback = False
        
        try:
            response = call_glm46(prompt, temperature=0.7, enable_reasoning=True)
        except Exception as e:
            error_msg = str(e)
            # æ£€æµ‹æ˜¯å¦æ˜¯å¹¶å‘é™åˆ¶æˆ–å…¶ä»–å¯æ¢å¤çš„é”™è¯¯ï¼Œå›é€€åˆ°glm-4.5-air
            # 429: å¹¶å‘é™åˆ¶ï¼›concurrency: å¹¶å‘ç›¸å…³é”™è¯¯ï¼›è°ƒç”¨glm4.6æ¨¡å‹å¤±è´¥: é€šç”¨å¤±è´¥
            is_recoverable_error = (
                "429" in error_msg or 
                "concurrency" in error_msg.lower() or 
                "è°ƒç”¨glm4.6æ¨¡å‹å¤±è´¥" in error_msg or
                "rate limit" in error_msg.lower() or
                "APIå¹¶å‘é™åˆ¶" in error_msg
            )
            
            if is_recoverable_error:
                print(f"[LLMInsight] GLM-4.6è°ƒç”¨å¤±è´¥: {error_msg}")
                print(f"[LLMInsight] å›é€€åˆ°GLM-4.5-airæ¨¡å‹...")
                try:
                    # ä½¿ç”¨glm-4.5-airï¼Œå¢åŠ è¶…æ—¶æ—¶é—´ä»¥é€‚åº”é•¿prompt
                    response = call_glm_45_air(prompt, temperature=0.7, timeout=120)
                    use_fallback = True
                    print(f"[LLMInsight] âœ“ å·²æˆåŠŸä½¿ç”¨GLM-4.5-airç”Ÿæˆæ´å¯Ÿ")
                except Exception as fallback_error:
                    # å¦‚æœå›é€€ä¹Ÿå¤±è´¥ï¼ŒæŠ›å‡ºè¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
                    raise Exception(
                        f"GLM-4.6å’ŒGLM-4.5-airéƒ½è°ƒç”¨å¤±è´¥ã€‚\n"
                        f"GLM-4.6é”™è¯¯: {error_msg}\n"
                        f"GLM-4.5-airé”™è¯¯: {str(fallback_error)}"
                    )
            else:
                # å…¶ä»–ç±»å‹çš„é”™è¯¯ç›´æ¥æŠ›å‡ºï¼Œä¸è¿›è¡Œå›é€€
                raise

        # è§£æJSONå“åº”
        try:
            # å°è¯•æå–JSONéƒ¨åˆ†
            if "```json" in response:
                json_str = response.split("```json")[1].split("```")[0].strip()
            elif "```" in response:
                json_str = response.split("```")[1].split("```")[0].strip()
            else:
                json_str = response.strip()

            insights = json.loads(json_str)
        except json.JSONDecodeError:
            # å¦‚æœè§£æå¤±è´¥ï¼ŒåŸºäºå“åº”åˆ›å»ºç»“æ„åŒ–æ´å¯Ÿ
            insights = {
                "sentiment_insight": "åŸºäºå›¾è¡¨åˆ†æï¼Œæƒ…æ„Ÿè¶‹åŠ¿æ˜¾ç¤ºæ•´ä½“æ€åŠ¿ç›¸å¯¹ç¨³å®šï¼Œéœ€è¦å…³æ³¨å¼‚å¸¸æ³¢åŠ¨ç‚¹ã€‚",
                "topic_insight": "ä¸»é¢˜æ¼”åŒ–åˆ†æè¡¨æ˜æ ¸å¿ƒè¯é¢˜æŒç»­æ´»è·ƒï¼Œæ–°å…´è¯é¢˜å‘ˆç°å¢é•¿è¶‹åŠ¿ã€‚",
                "geographic_insight": "åœ°ç†åˆ†å¸ƒåˆ†ææ˜¾ç¤ºçƒ­ç‚¹åŒºåŸŸé›†ä¸­ï¼ŒåŒºåŸŸå·®å¼‚ç‰¹å¾æ˜æ˜¾ã€‚",
                "cross_dimension_insight": "å‘å¸ƒè€…ç±»å‹åˆ†ææ˜¾ç¤ºä¸åŒç¾¤ä½“å½±å“åŠ›å·®å¼‚æ˜¾è‘—ï¼Œäº¤äº’æ¨¡å¼å¤šæ ·ã€‚",
                "summary_insight": response[:800] if response else "ç»¼åˆåˆ†æå·²å®Œæˆï¼Œå»ºè®®å…³æ³¨å›¾è¡¨ä¸­çš„å…³é”®å‘ç°ã€‚"
            }

        return insights
    
    def post(self, shared, prep_res, exec_res):
        """å¡«å……insightsåˆ°shared"""
        if "stage2_results" not in shared:
            shared["stage2_results"] = {}
        
        shared["stage2_results"]["insights"] = exec_res
        
        print(f"\n[LLMInsight] [OK] æ´å¯Ÿåˆ†æç”Ÿæˆå®Œæˆ")
        for key, value in exec_res.items():
            preview = value[:80] + "..." if len(value) > 80 else value
            print(f"  - {key}: {preview}")
        
        return "default"


# -----------------------------------------------------------------------------
# 4.3 Agentè‡ªä¸»è°ƒåº¦è·¯å¾„èŠ‚ç‚¹ (analysis_mode="agent")
# -----------------------------------------------------------------------------

class CollectToolsNode(Node):
    """
    å·¥å…·æ”¶é›†èŠ‚ç‚¹

    åŠŸèƒ½ï¼šé€šè¿‡MCPæœåŠ¡å™¨æ”¶é›†æ‰€æœ‰å¯ç”¨çš„åˆ†æå·¥å…·åˆ—è¡¨
    ç±»å‹ï¼šRegular Node
    æ§åˆ¶å‚æ•°ï¼šshared["config"]["tool_source"]

    MCPåè®®ç‰¹ç‚¹ï¼š
    - é€šè¿‡MCPåè®®åŠ¨æ€å‘ç°å’Œè°ƒç”¨åˆ†æå·¥å…·
    - æ”¯æŒå·¥å…·çš„åŠ¨æ€æ‰©å±•å’Œç‰ˆæœ¬ç®¡ç†
    - æ ‡å‡†åŒ–çš„å·¥å…·è°ƒç”¨æ¥å£
    """

    def prep(self, shared):
        """è¯»å–tool_sourceé…ç½®"""
        config = shared.get("config", {})
        tool_source = config.get("tool_source", "mcp")
        return {"tool_source": tool_source}

    def exec(self, prep_res):
        """é€šè¿‡MCPæœåŠ¡å™¨æ”¶é›†æ‰€æœ‰å¯ç”¨çš„åˆ†æå·¥å…·åˆ—è¡¨"""
        tool_source = prep_res["tool_source"]

        # å¯ç”¨MCPæ¨¡å¼
        from utils.mcp_client.mcp_client import set_mcp_mode, get_tools

        if tool_source == "mcp":
            set_mcp_mode(True)
            print(f"[CollectTools] ä½¿ç”¨MCPæ¨¡å¼è·å–å·¥å…·")
            tools = get_tools('utils/mcp_server')
        else:
            set_mcp_mode(False)
            print(f"[CollectTools] ä¸æ”¯æŒçš„å·¥å…·æº: {tool_source}")
            tools = []

        return {
            "tools": tools,
            "tool_source": tool_source,
            "tool_count": len(tools)
        }

    def post(self, shared, prep_res, exec_res):
        """å°†å·¥å…·å®šä¹‰å­˜å‚¨åˆ°shared"""
        if "agent" not in shared:
            shared["agent"] = {}

        shared["agent"]["available_tools"] = exec_res["tools"]
        shared["agent"]["execution_history"] = []
        shared["agent"]["current_iteration"] = 0
        shared["agent"]["is_finished"] = False
        shared["agent"]["tool_source"] = exec_res["tool_source"]  # è®°å½•ä½¿ç”¨çš„å·¥å…·æ¥æº

        config = shared.get("config", {})
        agent_config = config.get("agent_config", {})
        shared["agent"]["max_iterations"] = agent_config.get("max_iterations", 10)

        print(f"\n[CollectTools] [OK] æ”¶é›†åˆ° {exec_res['tool_count']} ä¸ªå¯ç”¨å·¥å…· ({exec_res['tool_source']}æ¨¡å¼)")

        # æŒ‰ç±»åˆ«æ˜¾ç¤ºå·¥å…·
        categories = {}
        for tool in exec_res["tools"]:
            cat = tool.get("category", "å…¶ä»–")
            if cat not in categories:
                categories[cat] = []
            categories[cat].append(tool["name"])

        for cat, tool_names in categories.items():
            print(f"  - {cat}: {', '.join(tool_names)}")

        return "default"


class DecisionToolsNode(Node):
    """
    å·¥å…·å†³ç­–èŠ‚ç‚¹

    åŠŸèƒ½ï¼šGLM4.6æ™ºèƒ½ä½“æ¨ç†å†³å®šä¸‹ä¸€æ­¥æ‰§è¡Œå“ªä¸ªåˆ†æå·¥å…·ï¼Œæˆ–åˆ¤æ–­åˆ†æå·²å……åˆ†
    ç±»å‹ï¼šRegular Node (LLM Call)
    æ¨¡å‹é…ç½®ï¼šGLM4.6 + æ¨ç†æ¨¡å¼ï¼ˆæ™ºèƒ½ä½“æ¨ç†ï¼‰
    å¾ªç¯å…¥å£ï¼šAgent Loopçš„å†³ç­–èµ·ç‚¹
    """
    
    def prep(self, shared):
        """è¯»å–æ•°æ®æ¦‚å†µã€å¯ç”¨å·¥å…·ã€æ‰§è¡Œå†å²ã€å½“å‰è¿­ä»£æ¬¡æ•°"""
        agent = shared.get("agent", {})
        
        return {
            "data_summary": agent.get("data_summary", ""),
            "available_tools": agent.get("available_tools", []),
            "execution_history": agent.get("execution_history", []),
            "current_iteration": agent.get("current_iteration", 0),
            "max_iterations": agent.get("max_iterations", 10)
        }
    
    def exec(self, prep_res):
        """æ„å»ºPromptè°ƒç”¨GLM4.6ï¼Œè·å–å†³ç­–ç»“æœ"""
        data_summary = prep_res["data_summary"]
        available_tools = prep_res["available_tools"]
        execution_history = prep_res["execution_history"]
        current_iteration = prep_res["current_iteration"]
        max_iterations = prep_res["max_iterations"]

        # æ„å»ºå·¥å…·åˆ—è¡¨æè¿°
        tools_description = []
        for tool in available_tools:
            tools_description.append(
                f"- {tool['name']} ({tool['category']}): {tool['description']}"
            )
        tools_text = "\n".join(tools_description)

        # æ„å»ºå®Œæ•´æ‰§è¡Œå†å²æè¿°
        if execution_history:
            # åˆ›å»ºå·²æ‰§è¡Œå·¥å…·çš„é›†åˆï¼Œä¾¿äºæ£€æµ‹é‡å¤
            executed_tools = set()
            history_items = []

            # æŒ‰æ—¶é—´é¡ºåºæ•´ç†æ‰€æœ‰æ‰§è¡Œè¿‡çš„å·¥å…·
            for i, item in enumerate(execution_history, 1):
                tool_name = item['tool_name']
                summary = item.get('summary', 'å·²æ‰§è¡Œ')
                has_chart = item.get('has_chart', False)
                has_data = item.get('has_data', False)
                error = item.get('error', False)

                # æ ‡è®°çŠ¶æ€å›¾æ ‡
                status_icon = "âœ…" if not error else "âŒ"
                chart_icon = "ğŸ“Š" if has_chart else ""
                data_icon = "ğŸ“‹" if has_data else ""

                history_items.append(
                    f"{i:2d}. {status_icon} **{tool_name}** {chart_icon}{data_icon}"
                )

                # è®°å½•å·²æ‰§è¡Œçš„å·¥å…·
                executed_tools.add(tool_name)

            # ç”Ÿæˆå†å²æ–‡æœ¬
            history_text = "\n".join(history_items)

            # åˆ›å»ºå·²æ‰§è¡Œå·¥å…·æ¸…å•ï¼Œé¿å…é‡å¤
            executed_tools_list = sorted(list(executed_tools))
            executed_tools_summary = f"å·²æ‰§è¡Œå·¥å…·æ¸…å• ({len(executed_tools_list)}ä¸ª): {', '.join(executed_tools_list)}"

        else:
            history_text = "å°šæœªæ‰§è¡Œä»»ä½•å·¥å…·"
            executed_tools_summary = "å·²æ‰§è¡Œå·¥å…·æ¸…å•: æ— "

        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„èˆ†æƒ…åˆ†ææ™ºèƒ½ä½“ï¼Œè´Ÿè´£å†³å®šä¸‹ä¸€æ­¥çš„åˆ†æåŠ¨ä½œã€‚è¯·è¿ç”¨ä½ çš„æ¨ç†èƒ½åŠ›ï¼ŒåŸºäºå½“å‰åˆ†æçŠ¶æ€åšå‡ºæœ€ä½³å†³ç­–ã€‚

## æ•°æ®æ¦‚å†µ
{data_summary}

## å¯ç”¨åˆ†æå·¥å…·
{tools_text}

## å®Œæ•´æ‰§è¡Œå†å²ï¼ˆæŒ‰æ—¶é—´é¡ºåºï¼‰
{history_text}

## å·¥å…·æ‰§è¡ŒçŠ¶æ€æ€»è§ˆ
{executed_tools_summary}

## å½“å‰çŠ¶æ€
- å½“å‰è¿­ä»£: {current_iteration + 1}/{max_iterations}
- å·²æ‰§è¡Œå·¥å…·æ•°: {len(execution_history)}
- å·²æ‰§è¡Œå·¥å…·è¦†ç›–ç‡: {len(executed_tools) if execution_history else 0}/{len(available_tools)}

## æ¨ç†å†³ç­–è¦æ±‚
è¯·è¿›è¡Œæ·±åº¦æ¨ç†åˆ†æï¼š

### 1. æ‰§è¡Œå†å²åˆ†æ
æ³¨æ„ä»¥ä¸‹å·¥å…·å·²ç»æ‰§è¡Œè¿‡ï¼š
{executed_tools_summary if execution_history else "æ— "}

### 2. åˆ†æå……åˆ†æ€§è¯„ä¼°
æ£€æŸ¥å››ä¸ªç»´åº¦çš„è¦†ç›–æƒ…å†µï¼š
- **æƒ…æ„Ÿåˆ†æç»´åº¦**ï¼šsentiment_* ç³»åˆ—å·¥å…·æ˜¯å¦å·²æ‰§è¡Œï¼Ÿ
- **ä¸»é¢˜åˆ†æç»´åº¦**ï¼štopic_* ç³»åˆ—å·¥å…·æ˜¯å¦å·²æ‰§è¡Œï¼Ÿ
- **åœ°ç†åˆ†æç»´åº¦**ï¼šgeographic_* ç³»åˆ—å·¥å…·æ˜¯å¦å·²æ‰§è¡Œï¼Ÿ
- **å¤šç»´äº¤äº’ç»´åº¦**ï¼špublisher_*, cross_*, influence_* å·¥å…·æ˜¯å¦å·²æ‰§è¡Œï¼Ÿ

### 3. å·¥å…·ä»·å€¼è¯„ä¼°
- **æ•°æ®ä»·å€¼ä¼˜å…ˆ**ï¼šé€‰æ‹©èƒ½æä¾›æ–°ç»Ÿè®¡æ•°æ®çš„å·¥å…·
- **å¯è§†åŒ–ä»·å€¼**ï¼šé€‰æ‹©èƒ½ç”Ÿæˆæ–°å›¾è¡¨çš„å·¥å…·
- **äº’è¡¥æ€§åˆ†æ**ï¼šé€‰æ‹©ä¸å·²æœ‰å·¥å…·å½¢æˆäº’è¡¥çš„å·¥å…·
- **é¿å…é‡å¤**ï¼šä¼˜å…ˆé€‰æ‹©æœªæ‰§è¡Œè¿‡çš„å·¥å…·

### 4. æ‰§è¡Œç­–ç•¥
- **ç»Ÿè®¡æ•°æ®å…ˆè¡Œ**ï¼šå…ˆæ‰§è¡Œ *_stats å·¥å…·è·å–åŸºç¡€æ•°æ®
- **å¯è§†åŒ–å·¥å…·åç»­**ï¼šå†æ‰§è¡Œ *_chart å·¥å…·ç”Ÿæˆå¯è§†åŒ–
- **ç»¼åˆå·¥å…·æœ€å**ï¼šcomprehensive_analysis ä½œä¸ºæ€»ç»“

## å†³ç­–è¾“å‡º
è¯·ä»¥JSONæ ¼å¼è¾“å‡ºä½ çš„æ¨ç†å†³ç­–ï¼š
```json
{{
    "thinking": "è¯¦ç»†æ¨ç†è¿‡ç¨‹ï¼š1)é‡å¤æ£€æµ‹ç»“æœ 2)ç»´åº¦è¦†ç›–åˆ†æ 3)å·¥å…·ä»·å€¼è¯„ä¼° 4)æœ€ç»ˆé€‰æ‹©ç†ç”±",
    "action": "executeæˆ–finish",
    "tool_name": "å·¥å…·åç§°ï¼ˆå¿…é¡»æ˜¯æœªæ‰§è¡Œçš„å·¥å…·ï¼‰",
    "reason": "é€‰æ‹©è¯¥å·¥å…·çš„å…·ä½“åŸå› å’Œé¢„æœŸåˆ†æä»·å€¼"
}}
```

**å»ºè®®**ï¼šä¼˜å…ˆé€‰æ‹©æœªæ‰§è¡Œè¿‡çš„å·¥å…·ä»¥è·å¾—æ›´å…¨é¢çš„åˆ†æç»“æœã€‚"""

        # ä½¿ç”¨GLM4.6æ¨¡å‹ï¼Œå¼€å¯æ¨ç†æ¨¡å¼
        response = call_glm46(prompt, temperature=0.6, enable_reasoning=True)

        # è§£æJSONå“åº”
        try:
            if "```json" in response:
                json_str = response.split("```json")[1].split("```")[0].strip()
            elif "```" in response:
                json_str = response.split("```")[1].split("```")[0].strip()
            else:
                json_str = response.strip()

            decision = json.loads(json_str)
        except json.JSONDecodeError:
            # è§£æå¤±è´¥ï¼Œé»˜è®¤ç»§ç»­æ‰§è¡Œ
            decision = {
                "action": "execute",
                "tool_name": "sentiment_distribution_stats",
                "reason": "GLM4.6å“åº”è§£æå¤±è´¥ï¼Œé»˜è®¤ä»æƒ…æ„Ÿåˆ†æå¼€å§‹"
            }

        return decision
    
    def post(self, shared, prep_res, exec_res):
        """è§£æå†³ç­–ï¼Œè¿”å›Action"""
        action = exec_res.get("action", "execute")

        if action == "finish":
            shared["agent"]["is_finished"] = True
            print(f"\n[DecisionTools] GLM4.6æ™ºèƒ½ä½“å†³å®š: åˆ†æå·²å……åˆ†ï¼Œç»“æŸå¾ªç¯")
            print(f"  æ¨ç†ç†ç”±: {exec_res.get('reason', 'æ— ')}")
            return "finish"
        else:
            tool_name = exec_res.get("tool_name", "")
            shared["agent"]["next_tool"] = tool_name
            shared["agent"]["next_tool_reason"] = exec_res.get("reason", "")

            print(f"\n[DecisionTools] GLM4.6æ™ºèƒ½ä½“å†³å®š: æ‰§è¡Œå·¥å…· {tool_name}")
            print(f"  æ¨ç†ç†ç”±: {exec_res.get('reason', 'æ— ')}")

            return "execute"


class ExecuteToolsNode(Node):
    """
    å·¥å…·æ‰§è¡ŒèŠ‚ç‚¹

    åŠŸèƒ½ï¼šé€šè¿‡MCPåè®®æ‰§è¡Œå†³ç­–èŠ‚ç‚¹é€‰å®šçš„åˆ†æå·¥å…·
    ç±»å‹ï¼šRegular Node

    MCPåè®®ç‰¹ç‚¹ï¼š
    - é€šè¿‡MCPåè®®è°ƒç”¨è¿œç¨‹åˆ†æå·¥å…·
    - æ ‡å‡†åŒ–çš„å·¥å…·è°ƒç”¨æ¥å£
    - æ”¯æŒå·¥å…·çš„åŠ¨æ€å‘ç°å’Œç‰ˆæœ¬ç®¡ç†
    """

    def prep(self, shared):
        """è¯»å–å†³ç­–ç»“æœä¸­çš„å·¥å…·åç§°å’Œæ•°æ®"""
        agent = shared.get("agent", {})
        blog_data = shared.get("data", {}).get("blog_data", [])
        tool_source = agent.get("tool_source", "mcp")
        enhanced_data_path = shared.get("config", {}).get("data_source", {}).get("enhanced_data_path", "")
        
        if not enhanced_data_path:
            print(f"[ExecuteTools] è­¦å‘Š: enhanced_data_path åœ¨ prep ä¸­ä¸ºç©º")
        else:
            print(f"[ExecuteTools] prep: enhanced_data_path={enhanced_data_path}")

        return {
            "tool_name": agent.get("next_tool", ""),
            "blog_data": blog_data,
            "tool_source": tool_source,
            "enhanced_data_path": enhanced_data_path
        }

    def exec(self, prep_res):
        """é€šè¿‡MCPåè®®è°ƒç”¨å¯¹åº”çš„åˆ†æå·¥å…·å‡½æ•°"""
        tool_name = prep_res["tool_name"]
        blog_data = prep_res["blog_data"]
        tool_source = prep_res["tool_source"]
        enhanced_data_path = prep_res.get("enhanced_data_path") or ""

        if not tool_name:
            return {"error": "æœªæŒ‡å®šå·¥å…·åç§°"}

        print(f"\n[ExecuteTools] æ‰§è¡Œå·¥å…·: {tool_name} ({tool_source}æ¨¡å¼)")

        # ä½¿ç”¨MCPå®¢æˆ·ç«¯è°ƒç”¨å·¥å…·
        from utils.mcp_client.mcp_client import call_tool

        try:
            # MCP server æ˜¯ç‹¬ç«‹å­è¿›ç¨‹ï¼šé€šè¿‡ç¯å¢ƒå˜é‡æŠŠå¢å¼ºæ•°æ®è·¯å¾„ä¼ ç»™å®ƒ
            # å¦åˆ™ mcp_server.get_blog_data() ä¼šè¿”å›ç©ºåˆ—è¡¨ï¼Œå¯¼è‡´"æ²¡æœ‰å¯ç»˜åˆ¶çš„æ•°æ®/æ²¡æœ‰åœ°åŒºæ•°æ®"ç­‰
            # ä¼˜å…ˆä½¿ç”¨ prep_res ä¸­çš„è·¯å¾„ï¼Œå¦‚æœä¸ºç©ºåˆ™ä½¿ç”¨ç¯å¢ƒå˜é‡ä¸­çš„è·¯å¾„
            if enhanced_data_path:
                abs_path = os.path.abspath(enhanced_data_path)
                os.environ["ENHANCED_DATA_PATH"] = abs_path
                print(f"[ExecuteTools] è®¾ç½® ENHANCED_DATA_PATH={abs_path}")
            else:
                # å¦‚æœæ²¡æœ‰ä» prep_res è·å–åˆ°è·¯å¾„ï¼Œå°è¯•ä»ç¯å¢ƒå˜é‡è·å–
                env_path = os.environ.get("ENHANCED_DATA_PATH")
                if env_path:
                    print(f"[ExecuteTools] ä½¿ç”¨ç¯å¢ƒå˜é‡ä¸­çš„ ENHANCED_DATA_PATH={env_path}")
                else:
                    print(f"[ExecuteTools] è­¦å‘Š: enhanced_data_path ä¸ºç©ºï¼Œç¯å¢ƒå˜é‡ä¸­ä¹Ÿæœªè®¾ç½®ï¼Œå¯èƒ½å¯¼è‡´æ•°æ®åŠ è½½å¤±è´¥")

            # å¯¹äºMCPå·¥å…·ï¼Œä¼ é€’æ­£ç¡®çš„æœåŠ¡å™¨è·¯å¾„ï¼Œä¸éœ€è¦ä¼ é€’blog_dataï¼ŒæœåŠ¡å™¨ä¼šè‡ªåŠ¨åŠ è½½
            result = call_tool('utils/mcp_server', tool_name, {})

            # è½¬æ¢MCPç»“æœä¸ºç»Ÿä¸€æ ¼å¼ï¼Œä¿è¯chartså­˜åœ¨ä¸”å«id/title/path
            charts = []
            if isinstance(result, dict):
                charts = result.get("charts") or []

                # å…¼å®¹åªæœ‰å•ä¸ªè·¯å¾„å­—æ®µçš„è¿”å›
                single_path = result.get("chart_path") or result.get("image_path") or result.get("file_path")
                if not charts and single_path:
                    charts = [{
                        "id": result.get("chart_id", tool_name),
                        "title": result.get("title", tool_name),
                        "path": single_path,
                        "file_path": single_path,
                        "type": result.get("type", "unknown"),
                        "description": result.get("description", ""),
                        "source_tool": tool_name
                    }]

                # è§„èŒƒåŒ–æ¯ä¸ªchartçš„å­—æ®µ
                normalized_charts = []
                for idx, ch in enumerate(charts):
                    if not isinstance(ch, dict):
                        continue
                    path = (
                        ch.get("path")
                        or ch.get("file_path")
                        or ch.get("chart_path")
                        or ch.get("image_path")
                        or ""
                    )
                    normalized_charts.append({
                        "id": ch.get("id") or f"{tool_name}_{idx}",
                        "title": ch.get("title") or tool_name,
                        "path": path,
                        "file_path": ch.get("file_path") or path,
                        "type": ch.get("type") or ch.get("chart_type") or "unknown",
                        "description": ch.get("description") or "",
                        "source_tool": ch.get("source_tool") or tool_name
                    })
                charts = normalized_charts

                final_result = {
                    "charts": charts,
                    "data": result if "data" not in result else result["data"],
                    "category": result.get("category") or self._get_tool_category(tool_name),
                    "summary": result.get("summary", f"MCPå·¥å…· {tool_name} æ‰§è¡Œå®Œæˆ")
                }
            else:
                # éå­—å…¸ç»“æœå…œåº•
                final_result = {
                    "charts": [],
                    "data": result,
                    "category": self._get_tool_category(tool_name),
                    "summary": f"MCPå·¥å…· {tool_name} æ‰§è¡Œå®Œæˆ"
                }
        except Exception as e:
            print(f"[ExecuteTools] MCPå·¥å…·è°ƒç”¨å¤±è´¥: {str(e)}")
            final_result = {"error": f"MCPå·¥å…·è°ƒç”¨å¤±è´¥: {str(e)}"}

        return {
            "tool_name": tool_name,
            "tool_source": tool_source,
            "result": final_result
        }

    def _get_tool_category(self, tool_name: str) -> str:
        """æ ¹æ®å·¥å…·åç§°æ¨æ–­ç±»åˆ«"""
        name_lower = tool_name.lower()
        if "sentiment" in name_lower:
            return "æƒ…æ„Ÿåˆ†æ"
        elif "topic" in name_lower:
            return "ä¸»é¢˜åˆ†æ"
        elif "geographic" in name_lower or "geo" in name_lower:
            return "åœ°ç†åˆ†æ"
        elif "publisher" in name_lower or "interaction" in name_lower:
            return "å¤šç»´äº¤äº’åˆ†æ"
        else:
            return "å…¶ä»–"

    def post(self, shared, prep_res, exec_res):
        """å­˜å‚¨ç»“æœï¼Œæ³¨å†Œå›¾è¡¨"""
        if "stage2_results" not in shared:
            shared["stage2_results"] = {
                "charts": [],
                "tables": [],
                "insights": {},
                "execution_log": {"tools_executed": []}
            }

        tool_name = exec_res["tool_name"]
        tool_source = exec_res["tool_source"]
        result = exec_res.get("result", {})
        result_payload = result
        if isinstance(result, dict):
            if isinstance(result.get("result"), dict):
                result_payload = result["result"]
            elif isinstance(result.get("data"), dict) and (
                "charts" in result["data"] or "summary" in result["data"]
            ):
                result_payload = result["data"]

        # è®°å½•æ‰§è¡Œçš„å·¥å…·
        shared["stage2_results"]["execution_log"]["tools_executed"].append(tool_name)

        # å¤„ç†é”™è¯¯æƒ…å†µ
        if "error" in result_payload:
            print(f"  [X] å·¥å…·æ‰§è¡Œå¤±è´¥: {result_payload['error']}")
            # å­˜å‚¨å¤±è´¥ç»“æœ
            shared["agent"]["last_tool_result"] = {
                "tool_name": tool_name,
                "summary": f"å·¥å…·æ‰§è¡Œå¤±è´¥: {result_payload['error']}",
                "has_chart": False,
                "has_data": False,
                "error": True
            }
            return "default"

        # å¤„ç†å›¾è¡¨
        if result_payload.get("charts"):
            shared["stage2_results"]["charts"].extend(result_payload["charts"])
            print(f"  [OK] ç”Ÿæˆ {len(result_payload['charts'])} ä¸ªå›¾è¡¨")

        # å¤„ç†æ•°æ®è¡¨æ ¼
        if result_payload.get("data"):
            shared["stage2_results"]["tables"].append({
                "id": tool_name,
                "title": result_payload.get("category", "") + " - " + tool_name,
                "data": result_payload["data"],
                "source_tool": tool_name,
                "source_type": tool_source  # è®°å½•æ•°æ®æ¥æº
            })
            print(f"  [OK] ç”Ÿæˆæ•°æ®è¡¨æ ¼")

        # å­˜å‚¨æ‰§è¡Œç»“æœä¾›ProcessResultNodeä½¿ç”¨
        shared["agent"]["last_tool_result"] = {
            "tool_name": tool_name,
            "tool_source": tool_source,
            "summary": result_payload.get("summary", "æ‰§è¡Œå®Œæˆ"),
            "has_chart": bool(result_payload.get("charts")),
            "has_data": bool(result_payload.get("data")),
            "error": False
        }

        return "default"


class ProcessResultNode(Node):
    """
    ç»“æœå¤„ç†èŠ‚ç‚¹
    
    åŠŸèƒ½ï¼šç®€å•åˆ†æå·¥å…·æ‰§è¡Œç»“æœï¼Œæ›´æ–°æ‰§è¡Œå†å²ï¼Œåˆ¤æ–­æ˜¯å¦ç»§ç»­å¾ªç¯
    ç±»å‹ï¼šRegular Node
    å¾ªç¯æ§åˆ¶ï¼šæ ¹æ®åˆ†æç»“æœå’Œè¿­ä»£æ¬¡æ•°å†³å®šæ˜¯å¦è¿”å›å†³ç­–èŠ‚ç‚¹
    """
    
    def prep(self, shared):
        """è¯»å–å·¥å…·æ‰§è¡Œç»“æœå’Œå½“å‰è¿­ä»£æ¬¡æ•°"""
        agent = shared.get("agent", {})
        
        return {
            "last_result": agent.get("last_tool_result", {}),
            "execution_history": agent.get("execution_history", []),
            "current_iteration": agent.get("current_iteration", 0),
            "max_iterations": agent.get("max_iterations", 10),
            "is_finished": agent.get("is_finished", False)
        }
    
    def exec(self, prep_res):
        """æ ¼å¼åŒ–ç»“æœã€æ›´æ–°è¿­ä»£è®¡æ•°"""
        last_result = prep_res["last_result"]
        execution_history = prep_res["execution_history"]
        current_iteration = prep_res["current_iteration"]
        max_iterations = prep_res["max_iterations"]
        is_finished = prep_res["is_finished"]
        
        # æ·»åŠ åˆ°æ‰§è¡Œå†å²
        if last_result:
            execution_history.append(last_result)
        
        # æ›´æ–°è¿­ä»£è®¡æ•°
        new_iteration = current_iteration + 1
        
        # åˆ¤æ–­æ˜¯å¦ç»§ç»­
        should_continue = (
            not is_finished and 
            new_iteration < max_iterations
        )
        
        return {
            "execution_history": execution_history,
            "new_iteration": new_iteration,
            "should_continue": should_continue,
            "reason": (
                "Agentåˆ¤æ–­åˆ†æå·²å……åˆ†" if is_finished else
                f"è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°({max_iterations})" if new_iteration >= max_iterations else
                "ç»§ç»­åˆ†æ"
            )
        }
    
    def post(self, shared, prep_res, exec_res):
        """æ›´æ–°çŠ¶æ€ï¼Œè¿”å›Action"""
        if "agent" not in shared:
            shared["agent"] = {}
        
        shared["agent"]["execution_history"] = exec_res["execution_history"]
        shared["agent"]["current_iteration"] = exec_res["new_iteration"]
        
        print(f"\n[ProcessResult] è¿­ä»£ {exec_res['new_iteration']}: {exec_res['reason']}")
        
        if exec_res["should_continue"]:
            return "continue"
        else:
            # ç»“æŸå¾ªç¯å‰ï¼Œç”Ÿæˆæ´å¯Ÿ
            print("[ProcessResult] Agentå¾ªç¯ç»“æŸï¼Œå‡†å¤‡ç”Ÿæˆæ´å¯Ÿåˆ†æ")
            return "finish"


# =============================================================================
# 5. é˜¶æ®µ3èŠ‚ç‚¹: æŠ¥å‘Šç”Ÿæˆ
# =============================================================================

